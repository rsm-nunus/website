[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nujoum Unus",
    "section": "",
    "text": "Nujoum Unus is pursuing her Masters in Business Analytics at the University of California, San Diego. A dedicated data science student with a passion for harnessing the power of machine learning, NLP, and generative AI to solve real-world business challenges; she excels in transforming complex datasets into strategic insights that drive decision-making and efficiency."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Nujoum Unus",
    "section": "Education",
    "text": "Education\nUniversity of California, San Diego | San Diego, CA MS Business Analytics | Aug 2024 - Dec 2025\nA.P.J. Abdul Kalam Technological University | Kerala, India B.Tech in Computer Science and Engineering | July 2019 - June 2023"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Nujoum Unus",
    "section": "Experience",
    "text": "Experience\nBriteCap Financial | Data Science Capstone | April 2025 - present\nTriton Power | Marketing Analyst Fellowship | April 2025 - present\nAutomation Engineer | TATA ELXSI | Nov 2023 - May 2024"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "WELCOME Y’ALL"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "hw1_questions.html",
    "href": "hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\n Background \nCharitable organizations often rely on fundraising letters to solicit donations, but little rigorous evidence has been available to guide how those letters should be designed. In a groundbreaking field experiment, economists Dean Karlan and John List set out to test how different framing strategies and financial incentives affect individual donation behavior.\nThe experiment, conducted in collaboration with a politically-oriented nonprofit organization, involved mailing 50,083 fundraising letters to previous donors. Crucially, the recipients were randomly assigned to different treatment groups, allowing the researchers to measure causal effects rather than mere correlations.\n Purpose of the Study \nKarlan and List aimed to answer a simple but important question:\n&gt; Do people give more when their donation is matched? And if so, does the size of the match matter?\nThey also explored additional behavioral levers commonly used in fundraising, such as challenge framing, suggested donation amounts, and goal-based appeals.\n Experimental Design \nThe letters fell into three broad treatment types:\n\nStandard Fundraising Letter (Control)\n\nA typical letter requesting support for the organization, with no additional incentives or matching language.\n\nMatching Grant Letter\n\nIncluded a paragraph stating that a leadership donor would match any contribution at one of three possible ratios:\n\n1:1 (every dollar given is doubled)\n\n2:1 (every dollar is tripled)\n\n3:1 (every dollar quadrupled)\n\n\nMatching offers also varied by threshold, i.e., the maximum amount the leadership donor would match:\n\n$25,000, $50,000, $100,000, or unstated.\n\nSuggested donation levels were tailored based on each recipient’s previous giving history:\n\nTheir highest previous gift\n\n1.25× their highest gift\n\n1.5× their highest gift\n\n\nChallenge Grant Letter\n\nFramed the offer as part of a collective effort or campaign challenge, appealing to urgency and social impact rather than pure match mechanics.\n\n\nBecause each component (match ratio, threshold, suggested donation amount) was randomized independently within the matching grant group, the experiment had a factorial design — allowing the researchers to isolate and measure the effects of each variable.\n Why This Matters \nAt the time of the study, fundraisers often relied on rules of thumb and anecdotes, lacking hard data on what actually drives giving. Karlan and List’s approach brought scientific rigor to the domain of nonprofit fundraising by:\n\nLeveraging random assignment to establish causality\nTesting commonly used marketing strategies under real-world conditions\nGenerating insights with practical implications for organizations seeking to raise more money\n\n Contribution to the Literature \nThis study represents one of the first large-scale natural field experiments in charitable giving. It moved beyond lab settings and survey experiments to observe real decisions involving real money. The results helped bridge the gap between behavioral economics and fundraising practice, offering evidence-backed recommendations on:\n\nThe efficacy of matching offers\n\nHow much match ratios influence behavior\n\nWhether people respond to thresholds or suggested amounts\n\nThe heterogeneous effects by donor characteristics and geography\n\n Project Overview \nIn this replication study, we use the same dataset provided by Karlan and List to:\n\nReproduce their key findings\nValidate the statistical robustness of their claims\nExplore new visualizations and simulations that illuminate the behavioral mechanisms at play\nReflect on what this experiment teaches us about human motivation, social framing, and economic incentives in the context of public goods\n\nThis report follows a structured analysis of donation likelihood, donation size, and how different dimensions of the match offer (ratio, threshold, framing) influence both."
  },
  {
    "objectID": "hw1_questions.html#introduction",
    "href": "hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\n Background \nCharitable organizations often rely on fundraising letters to solicit donations, but little rigorous evidence has been available to guide how those letters should be designed. In a groundbreaking field experiment, economists Dean Karlan and John List set out to test how different framing strategies and financial incentives affect individual donation behavior.\nThe experiment, conducted in collaboration with a politically-oriented nonprofit organization, involved mailing 50,083 fundraising letters to previous donors. Crucially, the recipients were randomly assigned to different treatment groups, allowing the researchers to measure causal effects rather than mere correlations.\n Purpose of the Study \nKarlan and List aimed to answer a simple but important question:\n&gt; Do people give more when their donation is matched? And if so, does the size of the match matter?\nThey also explored additional behavioral levers commonly used in fundraising, such as challenge framing, suggested donation amounts, and goal-based appeals.\n Experimental Design \nThe letters fell into three broad treatment types:\n\nStandard Fundraising Letter (Control)\n\nA typical letter requesting support for the organization, with no additional incentives or matching language.\n\nMatching Grant Letter\n\nIncluded a paragraph stating that a leadership donor would match any contribution at one of three possible ratios:\n\n1:1 (every dollar given is doubled)\n\n2:1 (every dollar is tripled)\n\n3:1 (every dollar quadrupled)\n\n\nMatching offers also varied by threshold, i.e., the maximum amount the leadership donor would match:\n\n$25,000, $50,000, $100,000, or unstated.\n\nSuggested donation levels were tailored based on each recipient’s previous giving history:\n\nTheir highest previous gift\n\n1.25× their highest gift\n\n1.5× their highest gift\n\n\nChallenge Grant Letter\n\nFramed the offer as part of a collective effort or campaign challenge, appealing to urgency and social impact rather than pure match mechanics.\n\n\nBecause each component (match ratio, threshold, suggested donation amount) was randomized independently within the matching grant group, the experiment had a factorial design — allowing the researchers to isolate and measure the effects of each variable.\n Why This Matters \nAt the time of the study, fundraisers often relied on rules of thumb and anecdotes, lacking hard data on what actually drives giving. Karlan and List’s approach brought scientific rigor to the domain of nonprofit fundraising by:\n\nLeveraging random assignment to establish causality\nTesting commonly used marketing strategies under real-world conditions\nGenerating insights with practical implications for organizations seeking to raise more money\n\n Contribution to the Literature \nThis study represents one of the first large-scale natural field experiments in charitable giving. It moved beyond lab settings and survey experiments to observe real decisions involving real money. The results helped bridge the gap between behavioral economics and fundraising practice, offering evidence-backed recommendations on:\n\nThe efficacy of matching offers\n\nHow much match ratios influence behavior\n\nWhether people respond to thresholds or suggested amounts\n\nThe heterogeneous effects by donor characteristics and geography\n\n Project Overview \nIn this replication study, we use the same dataset provided by Karlan and List to:\n\nReproduce their key findings\nValidate the statistical robustness of their claims\nExplore new visualizations and simulations that illuminate the behavioral mechanisms at play\nReflect on what this experiment teaches us about human motivation, social framing, and economic incentives in the context of public goods\n\nThis report follows a structured analysis of donation likelihood, donation size, and how different dimensions of the match offer (ratio, threshold, framing) influence both."
  },
  {
    "objectID": "hw1_questions.html#data",
    "href": "hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\nimport pandas as pd\ndf = pd.read_stata('karlan_list_2007.dta')\ndf.info()\ndf.describe()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50083 entries, 0 to 50082\nData columns (total 51 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   treatment           50083 non-null  int8    \n 1   control             50083 non-null  int8    \n 2   ratio               50083 non-null  category\n 3   ratio2              50083 non-null  int8    \n 4   ratio3              50083 non-null  int8    \n 5   size                50083 non-null  category\n 6   size25              50083 non-null  int8    \n 7   size50              50083 non-null  int8    \n 8   size100             50083 non-null  int8    \n 9   sizeno              50083 non-null  int8    \n 10  ask                 50083 non-null  category\n 11  askd1               50083 non-null  int8    \n 12  askd2               50083 non-null  int8    \n 13  askd3               50083 non-null  int8    \n 14  ask1                50083 non-null  int16   \n 15  ask2                50083 non-null  int16   \n 16  ask3                50083 non-null  int16   \n 17  amount              50083 non-null  float32 \n 18  gave                50083 non-null  int8    \n 19  amountchange        50083 non-null  float32 \n 20  hpa                 50083 non-null  float32 \n 21  ltmedmra            50083 non-null  int8    \n 22  freq                50083 non-null  int16   \n 23  years               50082 non-null  float64 \n 24  year5               50083 non-null  int8    \n 25  mrm2                50082 non-null  float64 \n 26  dormant             50083 non-null  int8    \n 27  female              48972 non-null  float64 \n 28  couple              48935 non-null  float64 \n 29  state50one          50083 non-null  int8    \n 30  nonlit              49631 non-null  float64 \n 31  cases               49631 non-null  float64 \n 32  statecnt            50083 non-null  float32 \n 33  stateresponse       50083 non-null  float32 \n 34  stateresponset      50083 non-null  float32 \n 35  stateresponsec      50080 non-null  float32 \n 36  stateresponsetminc  50080 non-null  float32 \n 37  perbush             50048 non-null  float32 \n 38  close25             50048 non-null  float64 \n 39  red0                50048 non-null  float64 \n 40  blue0               50048 non-null  float64 \n 41  redcty              49978 non-null  float64 \n 42  bluecty             49978 non-null  float64 \n 43  pwhite              48217 non-null  float32 \n 44  pblack              48047 non-null  float32 \n 45  page18_39           48217 non-null  float32 \n 46  ave_hh_sz           48221 non-null  float32 \n 47  median_hhincome     48209 non-null  float64 \n 48  powner              48214 non-null  float32 \n 49  psch_atlstba        48215 non-null  float32 \n 50  pop_propurban       48217 non-null  float32 \ndtypes: category(3), float32(16), float64(12), int16(4), int8(16)\nmemory usage: 8.9 MB\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168561\n0.135868\n0.103039\n0.378115\n22027.316665\n0.193405\n0.186599\n0.258654\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper).\nWe tested whether the treatment and control groups differed in prior donor behavior by comparing the number of months since last donation (mrm2). Both a two-sample t-test (t = 0.120, p = 0.905) and a linear regression of mrm2 ~ treatment (β = 0.0137, p = 0.905) confirm no statistically significant difference. This supports the randomization mechanism and matches Table 1 in Karlan and List (2007).\n\nfrom scipy import stats\n\n# Clean data: drop NAs\ndf_clean = df[[\"mrm2\", \"treatment\", \"control\"]].dropna()\n\n# Split groups\ntreat = df_clean[df_clean['treatment'] == 1]['mrm2']\ncontrol = df_clean[df_clean['control'] == 1]['mrm2']\n\n# Perform Welch's t-test (no assumption of equal variances)\nttest = stats.ttest_ind(treat, control, equal_var=False)\n\n# Print results\nprint(f\"T-test result: t = {ttest.statistic:.3f}, p = {ttest.pvalue:.3f}\")\n\nimport statsmodels.formula.api as smf\n\n# Regression of mrm2 on treatment\nmodel = smf.ols(\"mrm2 ~ treatment\", data=df_clean).fit()\nmodel.summary()\n\nT-test result: t = 0.120, p = 0.905\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nmrm2\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.01428\n\n\nDate:\nWed, 23 Apr 2025\nProb (F-statistic):\n0.905\n\n\nTime:\n22:29:36\nLog-Likelihood:\n-1.9585e+05\n\n\nNo. Observations:\n50082\nAIC:\n3.917e+05\n\n\nDf Residuals:\n50080\nBIC:\n3.917e+05\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n12.9981\n0.094\n138.979\n0.000\n12.815\n13.181\n\n\ntreatment\n0.0137\n0.115\n0.119\n0.905\n-0.211\n0.238\n\n\n\n\n\n\n\n\nOmnibus:\n8031.352\nDurbin-Watson:\n2.004\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n12471.135\n\n\nSkew:\n1.163\nProb(JB):\n0.00\n\n\nKurtosis:\n3.751\nCond. No.\n3.23\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n Interpretation \nWe assess balance between treatment and control groups using both statistical methods:\n\nT-Test:\n\nt = 0.120, p = 0.905\n\nResult: Not statistically significant\n\nRegression:\n\nCoefficient on treatment ≈ 0.014\n\np-value ≈ 0.905\n\n95% Confidence Interval: Includes zero\n\n\nThese results confirm no significant difference in mrm2 across groups, supporting the randomization mechanism. This aligns with Table 1 in Karlan & List (2007), where the group means were:\n\n\n\nGroup\nMean Months Since Last Donation\n\n\n\n\nTreatment\n13.012\n\n\nControl\n12.998"
  },
  {
    "objectID": "hw1_questions.html#experimental-results",
    "href": "hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\n\nimport matplotlib.pyplot as plt\ngave_by_group = df.groupby(\"treatment\")[\"gave\"].mean().reset_index()\ngave_by_group[\"group\"] = gave_by_group[\"treatment\"].map({0: \"Control\", 1: \"Treatment\"})\n\nplt.figure(figsize=(6, 4))\nplt.bar(gave_by_group[\"group\"], gave_by_group[\"gave\"], width=0.5)\nplt.title(\"Proportion Who Donated by Group\")\nplt.ylabel(\"Proportion Gave\")\nplt.ylim(0, gave_by_group[\"gave\"].max() + 0.01)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe compare the response rate (i.e., whether a donation was made) between treatment and control groups.\nThe bar plot below shows that the treatment group, who received a matching grant offer, donated at a higher rate than the control group, who received a standard letter.\nThis visual confirms the core finding in Karlan & List (2007): matching donations increased participation in charitable giving.\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n# Ensure binary outcome is correctly typed\ndf['gave'] = df['gave'].astype(int)\n\n# T-test: response rate (gave) between treatment and control groups\ngave_treat = df[df['treatment'] == 1]['gave']\ngave_control = df[df['control'] == 1]['gave']\nt_stat, p_val = ttest_ind(gave_treat, gave_control, equal_var=False)\n\n# Regression: response as a function of treatment\nreg_gave = smf.ols('gave ~ treatment', data=df).fit()\n\nt_stat, p_val, reg_gave.summary()\n\n(3.2094621908279835,\n 0.0013309823450914173,\n &lt;class 'statsmodels.iolib.summary.Summary'&gt;\n \"\"\"\n                             OLS Regression Results                            \n ==============================================================================\n Dep. Variable:                   gave   R-squared:                       0.000\n Model:                            OLS   Adj. R-squared:                  0.000\n Method:                 Least Squares   F-statistic:                     9.618\n Date:                Wed, 23 Apr 2025   Prob (F-statistic):            0.00193\n Time:                        22:29:36   Log-Likelihood:                 26630.\n No. Observations:               50083   AIC:                        -5.326e+04\n Df Residuals:                   50081   BIC:                        -5.324e+04\n Df Model:                           1                                         \n Covariance Type:            nonrobust                                         \n ==============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n ------------------------------------------------------------------------------\n Intercept      0.0179      0.001     16.225      0.000       0.016       0.020\n treatment      0.0042      0.001      3.101      0.002       0.002       0.007\n ==============================================================================\n Omnibus:                    59814.280   Durbin-Watson:                   2.005\n Prob(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\n Skew:                           6.740   Prob(JB):                         0.00\n Kurtosis:                      46.440   Cond. No.                         3.23\n ==============================================================================\n \n Notes:\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n \"\"\")\n\n\n Charitable Contribution Made \nWe examine whether receiving a matching donation offer increases the likelihood of making a charitable donation. This is measured using the binary variable gave, which equals 1 if a donation was made and 0 otherwise.\nWe use both a t-test and a bivariate linear regression to assess the difference in response rate between treatment and control groups.\n Results Summary \n\n\n\n\n\n\n\n\n\nMethod\nEffect Size\np-value\nInterpretation\n\n\n\n\nT-Test\nt = 3.21\n0.0013\nStatistically significant\n\n\nRegression\n+0.0042 (0.42%)\n0.002\nStatistically significant\n\n\n\n\nControl group donation rate ≈ 1.79%\n\nTreatment group donation rate ≈ 2.21%\n\nThese values match the response rates reported in Table 2A, Panel A of Karlan & List (2007).\n Table 2A (Panel A): Response Rate Comparison \n\n\n\nGroup\nResponse Rate\nStd. Error\n\n\n\n\nControl\n0.018\n(0.001)\n\n\nTreatment\n0.022\n(0.001)\n\n\nMatch 1:1\n0.021\n(0.001)\n\n\nMatch 2:1\n0.023\n(0.001)\n\n\nMatch 3:1\n0.023\n(0.001)\n\n\n\n\nSource: Karlan & List (2007), Table 2A, Panel A\n\n Interpretation \nEven a small increase in the likelihood of giving — about 0.4 percentage points — is statistically significant in a large-scale field experiment with over 50,000 individuals.\nThis result shows that: - Matching donations have a causal impact on behavior. - People are more likely to respond to donation appeals when told their gift will be matched. - The psychological effect (e.g., feeling of leverage, social validation, urgency) may be as important as the financial incentive.\nThus, matched donations are an effective strategy not just in economics but in behavioral design for charitable fundraising.\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\nimport statsmodels.api as sm\n\n# Prepare the variables\nX = sm.add_constant(df[\"treatment\"])\ny = df[\"gave\"]\n\n# Run the Probit regression\nprobit_model = sm.Probit(y, X)\nprobit_results = probit_model.fit()\n\nprobit_results.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\nProbit Regression Results\n\n\nDep. Variable:\ngave\nNo. Observations:\n50083\n\n\nModel:\nProbit\nDf Residuals:\n50081\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nWed, 23 Apr 2025\nPseudo R-squ.:\n0.0009783\n\n\nTime:\n22:29:37\nLog-Likelihood:\n-5030.5\n\n\nconverged:\nTrue\nLL-Null:\n-5035.4\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.001696\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-2.1001\n0.023\n-90.073\n0.000\n-2.146\n-2.054\n\n\ntreatment\n0.0868\n0.028\n3.113\n0.002\n0.032\n0.141\n\n\n\n\n\n Probit Regression: Impact of Matching Grant on Donation Likelihood \nTo replicate Table 3, Column (1) from Karlan & List (2007), we estimate a Probit model where the outcome is whether a donation was made (gave = 1) and the explanatory variable is assignment to treatment (treatment = 1).\n Our Probit Model Results \n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nz-value\np-value\n95% CI\n\n\n\n\nIntercept\n-2.100\n0.023\n-90.07\n&lt; 0.001\n[-2.146, -2.054]\n\n\nTreatment\n0.087\n0.028\n3.11\n0.002\n[0.032, 0.141]\n\n\n\n\nPseudo R²: 0.001\n\nObservations: 50,083\n\nThese results match the direction and significance of Table 3, Column (1) in the original study.\n Table 3: Primary Probit Regression Results from Karlan & List (2007) \n\n\n\n\n\n\n\n\n\nVariable\n(1) All\nStd. Err.\nSignificance\n\n\n\n\nTreatment\n0.004\n(0.001)\n***\n\n\nTreatment × 2:1 ratio\n0.002\n(0.002)\n\n\n\nTreatment × 3:1 ratio\n0.002\n(0.002)\n\n\n\nTreatment × $25,000 threshold\n-0.001\n(0.002)\n\n\n\nTreatment × $50,000 threshold\n0.000\n(0.002)\n\n\n\nTreatment × $100,000 threshold\n-0.000\n(0.002)\n\n\n\nTreatment × medium example amount\n0.001\n(0.002)\n\n\n\nTreatment × high example amount\n0.001\n(0.002)\n\n\n\nPseudo R²\n0.001\n\n\n\n\nObservations\n50,083\n\n\n\n\n\nNotes: - The paper reports marginal effects, whereas our Probit output gives latent index coefficients. - The magnitude of 0.004 in the paper corresponds to a marginal increase in probability of donating due to the treatment. - Our coefficient of 0.087 reflects the effect on the underlying propensity to give, which is standard in Probit estimation.\n Interpretation \nDespite a small effect size, the impact of being offered a matching donation is statistically significant. This suggests:\n\nEven subtle nudges, like framing a gift as matched by a leadership donor, can increase participation.\nThe result is economically meaningful due to the large sample size and real-world behavioral context.\n\nIn short: human generosity is sensitive to framing — and donors are more likely to act when they feel their gift has leverage.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\n\n# Subset the data for different match ratios\n# According to the dataset: ratio = '1', '2', '3' for 1:1, 2:1, 3:1\ndf_ratio = df[df[\"treatment\"] == 1].copy()\ndf_ratio[\"ratio\"] = df_ratio[\"ratio\"].astype(str)\n\n# Extract binary 'gave' for each match ratio group\ngave_1_1 = df_ratio[df_ratio[\"ratio\"] == \"1\"][\"gave\"]\ngave_2_1 = df_ratio[df_ratio[\"ratio\"] == \"2\"][\"gave\"]\ngave_3_1 = df_ratio[df_ratio[\"ratio\"] == \"3\"][\"gave\"]\n\n# Perform t-tests between match ratio groups\nttest_1_vs_2 = ttest_ind(gave_1_1, gave_2_1, equal_var=False)\nttest_1_vs_3 = ttest_ind(gave_1_1, gave_3_1, equal_var=False)\nttest_2_vs_3 = ttest_ind(gave_2_1, gave_3_1, equal_var=False)\n\nttest_1_vs_2, ttest_1_vs_3, ttest_2_vs_3\n\n(TtestResult(statistic=-0.965048975142932, pvalue=0.33453078237183076, df=22225.07770983836),\n TtestResult(statistic=-1.0150174470156275, pvalue=0.31010856527625774, df=22215.0529778684),\n TtestResult(statistic=-0.05011581369764474, pvalue=0.9600305476940865, df=22260.84918918778))\n\n\n Does Match Ratio Size Affect Donation Rates? \nWe investigate whether increasing the match ratio (from 1:1 to 2:1 to 3:1) has a statistically significant effect on the likelihood that someone donates.\nTo do this, we run a series of t-tests comparing donation rates (gave = 1) across match ratio groups, restricting the sample to individuals who received a matching offer.\n\n T-Test Results by Match Ratio \n\n\n\n\n\n\n\n\n\nComparison\nt-statistic\np-value\nInterpretation\n\n\n\n\n1:1 vs 2:1 match\n-0.965\n0.335\n❌ Not statistically significant\n\n\n1:1 vs 3:1 match\n-1.015\n0.310\n❌ Not statistically significant\n\n\n2:1 vs 3:1 match\n-0.050\n0.960\n❌ Not statistically significant\n\n\n\n Interpretation \nThese results show no significant difference in donation rates across the different match ratios. This means that:\n\nIncreasing the match multiplier from 1:1 to 2:1 or 3:1 does not lead to a higher likelihood of giving.\nThis supports the statement from Karlan & List (2007, p. 8):\n\n\n“The gift distributions across the various matching ratios are not significantly different from one another.”\n\nIn other words, people respond positively to the existence of a match, but not necessarily more when the match becomes more generous.\n Conclusion \nThe presence of a match appears to matter more than its magnitude. This suggests that:\n\nFraming and social cues — like simply saying “your gift will be matched” — may be more behaviorally powerful than the precise financial terms.\n\nThis insight is important for nonprofit fundraisers: focus on highlighting the match rather than inflating the ratio.\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\n\n# Ensure 'gave' is binary\ndf['gave'] = df['gave'].astype(int)\n\n# Create dummy variables for each match ratio\n# This is only for treatment group, so filter and prepare accordingly\ndf_ratio = df[df['treatment'] == 1].copy()\ndf_ratio['ratio'] = df_ratio['ratio'].astype(str)\n\n# Create dummy variables: ratio1, ratio2, ratio3\ndf_ratio['ratio1'] = (df_ratio['ratio'] == '1').astype(int)\ndf_ratio['ratio2'] = (df_ratio['ratio'] == '2').astype(int)\ndf_ratio['ratio3'] = (df_ratio['ratio'] == '3').astype(int)\n\n# Regression: gave ~ ratio1 + ratio2 + ratio3 (no intercept)\nimport statsmodels.api as sm\n\nX = df_ratio[['ratio1', 'ratio2', 'ratio3']]\ny = df_ratio['gave']\nmodel = sm.OLS(y, X).fit()\n\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ngave\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.6454\n\n\nDate:\nWed, 23 Apr 2025\nProb (F-statistic):\n0.524\n\n\nTime:\n22:29:37\nLog-Likelihood:\n16688.\n\n\nNo. Observations:\n33396\nAIC:\n-3.337e+04\n\n\nDf Residuals:\n33393\nBIC:\n-3.334e+04\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nratio1\n0.0207\n0.001\n14.912\n0.000\n0.018\n0.023\n\n\nratio2\n0.0226\n0.001\n16.267\n0.000\n0.020\n0.025\n\n\nratio3\n0.0227\n0.001\n16.335\n0.000\n0.020\n0.025\n\n\n\n\n\n\n\n\nOmnibus:\n38963.957\nDurbin-Watson:\n1.995\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n2506478.937\n\n\nSkew:\n6.511\nProb(JB):\n0.00\n\n\nKurtosis:\n43.394\nCond. No.\n1.00\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n Behavioral Insight: Why Match Size Doesn’t Matter (Much) \nThis regression shows that all forms of match ratios — 1:1, 2:1, and 3:1 — significantly increase the likelihood that someone donates, with donation rates clustering around 2%.\nHowever, the differences between match sizes are extremely small:\n\nPeople who saw a 1:1 match donated at a rate of 2.07%.\nThose who saw a 2:1 match gave at 2.26%.\nWith a 3:1 match, the rate was 2.27%.\n\nThese results suggest that once a match is present, increasing its generosity has little additional impact. In other words:\n\nIt’s the existence of the match that matters, not its size.\n\nThis behavior aligns with theories in behavioral economics: - The match acts as a signal of social proof or endorsement. - It may create a sense of urgency or leverage (“my donation matters more”). - But donors aren’t particularly sensitive to how generous the match is — at least not in terms of deciding whether or not to give.\n Implication for Fundraising \nFrom a practical standpoint, this means that: - Fundraisers don’t need to offer high match ratios to see results. - A simple, clearly communicated 1:1 match may be just as effective as a 3:1 match in increasing participation.\nThis finding reinforces the power of framing and perception in influencing human behavior.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n# Compute the actual mean response (gave) for each ratio group directly from the data\nmean_1_1 = df_ratio[df_ratio[\"ratio\"] == \"1\"][\"gave\"].mean()\nmean_2_1 = df_ratio[df_ratio[\"ratio\"] == \"2\"][\"gave\"].mean()\nmean_3_1 = df_ratio[df_ratio[\"ratio\"] == \"3\"][\"gave\"].mean()\n\n# Calculate differences in response rates\ndiff_2_1_vs_1_1 = mean_2_1 - mean_1_1\ndiff_3_1_vs_2_1 = mean_3_1 - mean_2_1\n\n# Extract coefficients from regression model\ncoef_1_1 = model.params[\"ratio1\"]\ncoef_2_1 = model.params[\"ratio2\"]\ncoef_3_1 = model.params[\"ratio3\"]\n\n# Calculate differences in coefficients\ncoef_diff_2_1_vs_1_1 = coef_2_1 - coef_1_1\ncoef_diff_3_1_vs_2_1 = coef_3_1 - coef_2_1\n\n(mean_1_1, mean_2_1, mean_3_1,\n diff_2_1_vs_1_1, diff_3_1_vs_2_1,\n coef_diff_2_1_vs_1_1, coef_diff_3_1_vs_2_1)\n\n(0.020749124225276205,\n 0.0226333752469912,\n 0.022733399227244138,\n 0.0018842510217149944,\n 0.00010002398025293902,\n 0.0018842510217149805,\n 0.00010002398025296677)\n\n\n Comparing Response Rates Across Match Ratios \nWe examine how the size of the match (1:1 vs. 2:1 vs. 3:1) influences the probability that an individual makes a donation. We do this in two ways:\n\nDirectly from the data by calculating average donation rates within each match group.\nFrom the fitted coefficients of a regression on dummy variables for each ratio.\n\n\n Response Rate Differences \n\n\n\n\n\n\n\n\nComparison\nDirect from Data\nFrom Regression Coefficients\n\n\n\n\n2:1 vs 1:1 match\n0.00188 (0.19%)\n0.00188 (0.19%)\n\n\n3:1 vs 2:1 match\n0.00010 (0.01%)\n0.00010 (0.01%)\n\n\n\n\nThese differences represent increases in the probability of donating when moving from one match ratio to a higher one.\nThe results are identical across both methods, which supports the robustness of the findings.\n\n Interpretation \n\nMoving from a 1:1 to 2:1 match slightly increases donation rates by about 0.19 percentage points.\nIncreasing from a 2:1 to a 3:1 match has a negligible effect — only 0.01 percentage points.\nThese differences are statistically very small and are unlikely to be meaningful in practice.\n\n Conclusion \nOur analysis shows that:\n\nOnce a match is introduced, increasing the match ratio does not meaningfully increase the likelihood of giving.\n\nThis confirms the finding from Karlan & List (2007):\n\n“The gift distributions across the various matching ratios are not significantly different from one another.”\n\nIn short, it’s the presence of a match offer — not its generosity — that influences donor behavior.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\n\n# Run a t-test on the amount given between treatment and control groups\namount_treat = df[df['treatment'] == 1]['amount']\namount_control = df[df['control'] == 1]['amount']\namount_ttest = ttest_ind(amount_treat, amount_control, equal_var=False)\n\n# Run a bivariate linear regression: amount ~ treatment\namount_reg = smf.ols('amount ~ treatment', data=df).fit()\n\namount_ttest.statistic, amount_ttest.pvalue, amount_reg.summary()\n\n(1.9182618934467577,\n 0.05508566528918335,\n &lt;class 'statsmodels.iolib.summary.Summary'&gt;\n \"\"\"\n                             OLS Regression Results                            \n ==============================================================================\n Dep. Variable:                 amount   R-squared:                       0.000\n Model:                            OLS   Adj. R-squared:                  0.000\n Method:                 Least Squares   F-statistic:                     3.461\n Date:                Wed, 23 Apr 2025   Prob (F-statistic):             0.0628\n Time:                        22:29:37   Log-Likelihood:            -1.7946e+05\n No. Observations:               50083   AIC:                         3.589e+05\n Df Residuals:                   50081   BIC:                         3.589e+05\n Df Model:                           1                                         \n Covariance Type:            nonrobust                                         \n ==============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n ------------------------------------------------------------------------------\n Intercept      0.8133      0.067     12.063      0.000       0.681       0.945\n treatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n ==============================================================================\n Omnibus:                    96861.113   Durbin-Watson:                   2.008\n Prob(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\n Skew:                          15.297   Prob(JB):                         0.00\n Kurtosis:                     341.269   Cond. No.                         3.23\n ==============================================================================\n \n Notes:\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n \"\"\")\n\n\n Size of Charitable Contribution \nWe tested whether receiving a matching donation offer affects the amount donated using a t-test and linear regression:\n Results \n\n\n\n\n\n\n\n\n\nMethod\nTreatment Effect\np-value\nConclusion\n\n\n\n\nT-Test\n+$0.15\n0.055\n🔸 Marginally not significant\n\n\nRegression\n+$0.15\n0.063\n🔸 Suggestive but inconclusive\n\n\n\n\nControl group average: ~$0.81\n\nTreatment group average: ~$0.96\n\n Interpretation \n\nThe treatment group gave slightly more, but the difference is not statistically significant at the 5% level.\nThis suggests that while match offers increase participation, they have a much smaller effect on how much people give.\n\n Takeaway \n\nMatching donations may encourage more people to give, but do not substantially increase donation size.\n\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\n\n# Limit the data to only those who made a donation (amount &gt; 0)\ndf_positive = df[df['amount'] &gt; 0].copy()\n\n# T-test for amount among donors only\namount_treat_pos = df_positive[df_positive['treatment'] == 1]['amount']\namount_control_pos = df_positive[df_positive['control'] == 1]['amount']\namount_ttest_pos = ttest_ind(amount_treat_pos, amount_control_pos, equal_var=False)\n\n# Regression: amount ~ treatment (for donors only)\namount_reg_pos = smf.ols('amount ~ treatment', data=df_positive).fit()\n\namount_ttest_pos.statistic, amount_ttest_pos.pvalue, amount_reg_pos.summary()\n\n(-0.5846089794983359,\n 0.5590471865673547,\n &lt;class 'statsmodels.iolib.summary.Summary'&gt;\n \"\"\"\n                             OLS Regression Results                            \n ==============================================================================\n Dep. Variable:                 amount   R-squared:                       0.000\n Model:                            OLS   Adj. R-squared:                 -0.001\n Method:                 Least Squares   F-statistic:                    0.3374\n Date:                Wed, 23 Apr 2025   Prob (F-statistic):              0.561\n Time:                        22:29:37   Log-Likelihood:                -5326.8\n No. Observations:                1034   AIC:                         1.066e+04\n Df Residuals:                    1032   BIC:                         1.067e+04\n Df Model:                           1                                         \n Covariance Type:            nonrobust                                         \n ==============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n ------------------------------------------------------------------------------\n Intercept     45.5403      2.423     18.792      0.000      40.785      50.296\n treatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n ==============================================================================\n Omnibus:                      587.258   Durbin-Watson:                   2.031\n Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\n Skew:                           2.464   Prob(JB):                         0.00\n Kurtosis:                      13.307   Cond. No.                         3.49\n ==============================================================================\n \n Notes:\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n \"\"\")\n\n\n Conditional Donation Amount: Among Donors Only \nTo isolate the effect of treatment on the amount given, we restrict the sample to only those individuals who made a donation (amount &gt; 0).\nWe use both a t-test and a bivariate regression (amount ~ treatment) to compare average donation sizes between treatment and control groups.\n Results Summary \n\n\n\n\n\n\n\n\n\nMethod\nTreatment Effect\np-value\nConclusion\n\n\n\n\nT-Test (donors only)\nt = -0.58\n0.559\n❌ Not statistically significant\n\n\nRegression\n-$1.67\n0.561\n❌ Not statistically significant\n\n\n\n\nControl group average donation: ~$45.54\n\nTreatment group average donation: ~$43.87\n\n Interpretation \n\nThe treatment group donated slightly less on average, but the difference is not statistically meaningful.\nThis suggests that while the match offer encourages more people to donate, it does not increase donation size among those who would give anyway.\nBecause we only include those who donated, the treatment effect here is not causal — it’s conditional and may suffer from selection bias.\n\n Conclusion \n\nMatched donations are effective at increasing the number of donors, but not the amount donated by each donor — at least among those who already choose to give.\n\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot.\n\nimport matplotlib.pyplot as plt\n\n# Filter to donors only\ndf_donors = df[df[\"amount\"] &gt; 0]\n\n# Separate treatment and control donors\ntreat_donors = df_donors[df_donors[\"treatment\"] == 1][\"amount\"]\ncontrol_donors = df_donors[df_donors[\"control\"] == 1][\"amount\"]\n\n# Calculate means\nmean_treat = treat_donors.mean()\nmean_control = control_donors.mean()\n\n# Create side-by-side histograms\nfig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n\n# Control group plot\naxes[0].hist(control_donors, bins=30, color=\"skyblue\", edgecolor=\"black\")\naxes[0].axvline(mean_control, color=\"red\", linestyle=\"--\", label=f\"Mean = ${mean_control:.2f}\")\naxes[0].set_title(\"Control Group Donations\")\naxes[0].set_xlabel(\"Donation Amount\")\naxes[0].set_ylabel(\"Frequency\")\naxes[0].legend()\n\n# Treatment group plot\naxes[1].hist(treat_donors, bins=30, color=\"lightgreen\", edgecolor=\"black\")\naxes[1].axvline(mean_treat, color=\"red\", linestyle=\"--\", label=f\"Mean = ${mean_treat:.2f}\")\naxes[1].set_title(\"Treatment Group Donations\")\naxes[1].set_xlabel(\"Donation Amount\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n Distribution of Donation Amounts Among Donors \nWe now focus on individuals who actually made a donation (amount &gt; 0) to analyze how much they gave, and whether the treatment group (those offered a matching donation) gave more than the control group.\nWe visualize the distribution of donation amounts with two histograms — one for each group — and include a red dashed line indicating the average donation in each.\n Interpretation \n\nBoth distributions are heavily right-skewed, which is common in charitable giving: most donors give modest amounts, but a few give significantly more.\nThe average donation in the control group was about $45.54, while the treatment group averaged $43.87.\nThis difference is not statistically significant, as confirmed by both a t-test and a regression limited to donors.\n\n What Did We Learn? \n\nWhile the matching donation offer increases the probability of donating, it does not increase the donation amount among those who choose to give.\nIn fact, the average donation in the treatment group is slightly lower, though the difference is not meaningful.\n\n Important Caveat \nThis analysis is based only on people who gave, so the treatment coefficient does not have a causal interpretation here. This subset is not randomly assigned — it’s a selected group, which may differ systematically between treatment and control.\n Fundraising Implication \n\nMatching offers are powerful tools to increase participation, but they do not necessarily lead to larger individual gifts.\n\nTo increase average donation size, fundraisers may need additional tactics — such as suggested donation levels, tiered match thresholds, or social proof."
  },
  {
    "objectID": "hw1_questions.html#simulation-experiment",
    "href": "hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Extract donation amounts for control and treatment\ncontrol_data = df[df[\"control\"] == 1][\"amount\"]\ntreatment_data = df[df[\"treatment\"] == 1][\"amount\"]\n\n# Simulate draws from each distribution\nnp.random.seed(42)\nsim_control = np.random.choice(control_data, size=100_000, replace=True)\nsim_treatment = np.random.choice(treatment_data, size=10_000, replace=True)\n\n# Calculate 10,000 differences between treatment and control draws\nsim_control_subset = np.random.choice(sim_control, size=10_000, replace=False)\ndiffs = sim_treatment - sim_control_subset\n\n# Compute cumulative average of differences\ncumulative_avg = np.cumsum(diffs) / np.arange(1, len(diffs) + 1)\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label=\"Cumulative Average Difference\")\nplt.axhline(y=np.mean(treatment_data) - np.mean(control_data), color=\"red\", linestyle=\"--\", label=\"True Mean Difference\")\nplt.title(\"Cumulative Average of Treatment-Control Differences\")\nplt.xlabel(\"Number of Draws\")\nplt.ylabel(\"Cumulative Average Difference in Donation Amount\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n Simulated Cumulative Average Differences \nTo better understand the behavior of sample averages and connect to the concepts from our first class (Slide 43), we simulate the cumulative effect of donation differences between the treatment and control groups.\n Simulation Setup \n\nWe simulate 100,000 random draws from the control group donation distribution.\nWe simulate 10,000 random draws from the treatment group.\nFor each of the 10,000 pairs, we calculate the difference: treatment - control.\nWe then compute the cumulative average of these 10,000 differences.\n\n Plot Interpretation \nThe plot below shows:\n\nA blue line representing the cumulative average of the simulated differences.\nA red dashed line indicating the true difference in means between treatment and control groups (calculated from the full dataset).\n\nAs the number of draws increases, the cumulative average approaches the true difference.\nThis illustrates the Law of Large Numbers: with enough data, sample-based estimates converge to the population value.\n What We Learnt \n\nThis simulation confirms that even in noisy, skewed data like donations, repeated sampling yields reliable estimates.\n\nIt also demonstrates that the difference in means we compute from data is not just a fluke — it’s what we’d expect if we sampled repeatedly from the same distributions.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”\n\n# Define a function to simulate mean differences for a given sample size\ndef simulate_differences(sample_size, n_reps=1000):\n    differences = []\n    for _ in range(n_reps):\n        sample_control = np.random.choice(control_data, size=sample_size, replace=True)\n        sample_treatment = np.random.choice(treatment_data, size=sample_size, replace=True)\n        differences.append(np.mean(sample_treatment) - np.mean(sample_control))\n    return differences\n\n# Simulate for each sample size\nnp.random.seed(42)\nsizes = [50, 200, 500, 1000]\nsimulated_results = {size: simulate_differences(size) for size in sizes}\n\n# Plot histograms\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i, size in enumerate(sizes):\n    axes[i].hist(simulated_results[size], bins=30, color='lightgray', edgecolor='black')\n    axes[i].axvline(0, color='red', linestyle='--', label=\"Zero\")\n    axes[i].set_title(f\"Sample Size = {size}\")\n    axes[i].set_xlabel(\"Mean Difference (Treatment - Control)\")\n    axes[i].set_ylabel(\"Frequency\")\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n Sampling Distributions at Different Sample Sizes \nTo mirror the exercise from Slide 44 of our first class, we simulate the sampling distribution of the mean difference in donation amount between the treatment and control groups.\nFor each of four different sample sizes — 50, 200, 500, and 1000 — we:\n\nDraw n observations from each group.\nCompute the difference in mean donation: treatment - control.\nRepeat the process 1,000 times.\nPlot the histogram of those 1,000 average differences.\n\n Histograms of Simulated Mean Differences \nEach plot includes a red dashed line at zero, representing the null hypothesis of no effect.\n Interpretation by Sample Size \n\nn = 50: The distribution is wide and noisy. Zero is near the center, meaning we can’t confidently detect an effect.\nn = 200: The distribution begins to narrow. Zero is still well within the range of plausible outcomes.\nn = 500: The histogram becomes more concentrated. The true effect begins to emerge, and zero starts shifting toward the tails.\nn = 1000: The distribution is tightly centered. Zero lies in the tail, indicating that the true average difference is likely not zero.\n\n Conclusion \n\nAs sample size increases, the sampling distribution of the mean difference becomes narrower and more centered around the true population effect.\n\nThis exercise demonstrates: - The Law of Large Numbers: larger samples produce more stable estimates. - The power of simulation for understanding uncertainty and inference. - Why small samples often yield inconclusive or misleading results.\nThese plots reinforce that while we may see noisy or overlapping outcomes in small samples, with enough data, we get closer to the truth."
  },
  {
    "objectID": "a_b_test.html",
    "href": "a_b_test.html",
    "title": "Experimental Results",
    "section": "",
    "text": "import pandas as pd\n\n\ndf = pd.read_stata('karlan_list_2007.dta')\n\n\ndf.describe()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168561\n0.135868\n0.103039\n0.378115\n22027.316665\n0.193405\n0.186599\n0.258654\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50083 entries, 0 to 50082\nData columns (total 51 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   treatment           50083 non-null  int8    \n 1   control             50083 non-null  int8    \n 2   ratio               50083 non-null  category\n 3   ratio2              50083 non-null  int8    \n 4   ratio3              50083 non-null  int8    \n 5   size                50083 non-null  category\n 6   size25              50083 non-null  int8    \n 7   size50              50083 non-null  int8    \n 8   size100             50083 non-null  int8    \n 9   sizeno              50083 non-null  int8    \n 10  ask                 50083 non-null  category\n 11  askd1               50083 non-null  int8    \n 12  askd2               50083 non-null  int8    \n 13  askd3               50083 non-null  int8    \n 14  ask1                50083 non-null  int16   \n 15  ask2                50083 non-null  int16   \n 16  ask3                50083 non-null  int16   \n 17  amount              50083 non-null  float32 \n 18  gave                50083 non-null  int8    \n 19  amountchange        50083 non-null  float32 \n 20  hpa                 50083 non-null  float32 \n 21  ltmedmra            50083 non-null  int8    \n 22  freq                50083 non-null  int16   \n 23  years               50082 non-null  float64 \n 24  year5               50083 non-null  int8    \n 25  mrm2                50082 non-null  float64 \n 26  dormant             50083 non-null  int8    \n 27  female              48972 non-null  float64 \n 28  couple              48935 non-null  float64 \n 29  state50one          50083 non-null  int8    \n 30  nonlit              49631 non-null  float64 \n 31  cases               49631 non-null  float64 \n 32  statecnt            50083 non-null  float32 \n 33  stateresponse       50083 non-null  float32 \n 34  stateresponset      50083 non-null  float32 \n 35  stateresponsec      50080 non-null  float32 \n 36  stateresponsetminc  50080 non-null  float32 \n 37  perbush             50048 non-null  float32 \n 38  close25             50048 non-null  float64 \n 39  red0                50048 non-null  float64 \n 40  blue0               50048 non-null  float64 \n 41  redcty              49978 non-null  float64 \n 42  bluecty             49978 non-null  float64 \n 43  pwhite              48217 non-null  float32 \n 44  pblack              48047 non-null  float32 \n 45  page18_39           48217 non-null  float32 \n 46  ave_hh_sz           48221 non-null  float32 \n 47  median_hhincome     48209 non-null  float64 \n 48  powner              48214 non-null  float32 \n 49  psch_atlstba        48215 non-null  float32 \n 50  pop_propurban       48217 non-null  float32 \ndtypes: category(3), float32(16), float64(12), int16(4), int8(16)\nmemory usage: 8.9 MB\n\n\n\nfrom scipy import stats\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Separate data into treatment and control groups\ntreatment_group = df[df['treatment'] == 1]\ncontrol_group = df[df['control'] == 1]\n\n# T-test: mrm2 (months since last donation)\nttest_result = stats.ttest_ind(treatment_group['mrm2'].dropna(), control_group['mrm2'].dropna(), equal_var=False)\n\n# Linear regression: mrm2 ~ treatment\ndf_reg = df[['mrm2', 'treatment']].dropna()\nreg_result = smf.ols('mrm2 ~ treatment', data=df_reg).fit()\nttest_result, reg_result.summary()\n\n(TtestResult(statistic=0.11953155228177251, pvalue=0.9048549631450832, df=33394.47581389535),\n &lt;class 'statsmodels.iolib.summary.Summary'&gt;\n \"\"\"\n                             OLS Regression Results                            \n ==============================================================================\n Dep. Variable:                   mrm2   R-squared:                       0.000\n Model:                            OLS   Adj. R-squared:                 -0.000\n Method:                 Least Squares   F-statistic:                   0.01428\n Date:                Wed, 23 Apr 2025   Prob (F-statistic):              0.905\n Time:                        15:29:10   Log-Likelihood:            -1.9585e+05\n No. Observations:               50082   AIC:                         3.917e+05\n Df Residuals:                   50080   BIC:                         3.917e+05\n Df Model:                           1                                         \n Covariance Type:            nonrobust                                         \n ==============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n ------------------------------------------------------------------------------\n Intercept     12.9981      0.094    138.979      0.000      12.815      13.181\n treatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n ==============================================================================\n Omnibus:                     8031.352   Durbin-Watson:                   2.004\n Prob(Omnibus):                  0.000   Jarque-Bera (JB):            12471.135\n Skew:                           1.163   Prob(JB):                         0.00\n Kurtosis:                       3.751   Cond. No.                         3.23\n ==============================================================================\n \n Notes:\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n \"\"\")\n\n\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\n\nimport matplotlib.pyplot as plt\ngave_by_group = df.groupby(\"treatment\")[\"gave\"].mean().reset_index()\ngave_by_group[\"group\"] = gave_by_group[\"treatment\"].map({0: \"Control\", 1: \"Treatment\"})\n\nplt.figure(figsize=(6, 4))\nplt.bar(gave_by_group[\"group\"], gave_by_group[\"gave\"], width=0.5)\nplt.title(\"Proportion Who Donated by Group\")\nplt.ylabel(\"Proportion Gave\")\nplt.ylim(0, gave_by_group[\"gave\"].max() + 0.01)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\n\n# Re-import libraries and reload data due to kernel reset\nimport pandas as pd\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n# Load data\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Ensure binary outcome is correctly typed\ndf['gave'] = df['gave'].astype(int)\n\n# T-test: response rate (gave) between treatment and control groups\ngave_treat = df[df['treatment'] == 1]['gave']\ngave_control = df[df['control'] == 1]['gave']\nt_stat, p_val = ttest_ind(gave_treat, gave_control, equal_var=False)\n\n# Regression: response as a function of treatment\nreg_gave = smf.ols('gave ~ treatment', data=df).fit()\n\nt_stat, p_val, reg_gave.summary()\n\n(3.2094621908279835,\n 0.0013309823450914173,\n &lt;class 'statsmodels.iolib.summary.Summary'&gt;\n \"\"\"\n                             OLS Regression Results                            \n ==============================================================================\n Dep. Variable:                   gave   R-squared:                       0.000\n Model:                            OLS   Adj. R-squared:                  0.000\n Method:                 Least Squares   F-statistic:                     9.618\n Date:                Wed, 23 Apr 2025   Prob (F-statistic):            0.00193\n Time:                        21:04:50   Log-Likelihood:                 26630.\n No. Observations:               50083   AIC:                        -5.326e+04\n Df Residuals:                   50081   BIC:                        -5.324e+04\n Df Model:                           1                                         \n Covariance Type:            nonrobust                                         \n ==============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n ------------------------------------------------------------------------------\n Intercept      0.0179      0.001     16.225      0.000       0.016       0.020\n treatment      0.0042      0.001      3.101      0.002       0.002       0.007\n ==============================================================================\n Omnibus:                    59814.280   Durbin-Watson:                   2.005\n Prob(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\n Skew:                           6.740   Prob(JB):                         0.00\n Kurtosis:                      46.440   Cond. No.                         3.23\n ==============================================================================\n \n Notes:\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n \"\"\")\n\n\n\nimport statsmodels.api as sm\n\n# Prepare the variables\nX = sm.add_constant(df[\"treatment\"])\ny = df[\"gave\"]\n\n# Run the Probit regression\nprobit_model = sm.Probit(y, X)\nprobit_results = probit_model.fit()\n\nprobit_results.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\nProbit Regression Results\n\n\nDep. Variable:\ngave\nNo. Observations:\n50083\n\n\nModel:\nProbit\nDf Residuals:\n50081\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nWed, 23 Apr 2025\nPseudo R-squ.:\n0.0009783\n\n\nTime:\n21:22:42\nLog-Likelihood:\n-5030.5\n\n\nconverged:\nTrue\nLL-Null:\n-5035.4\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.001696\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-2.1001\n0.023\n-90.073\n0.000\n-2.146\n-2.054\n\n\ntreatment\n0.0868\n0.028\n3.113\n0.002\n0.032\n0.141\n\n\n\n\n\n\n# Subset the data for different match ratios\n# According to the dataset: ratio = '1', '2', '3' for 1:1, 2:1, 3:1\ndf_ratio = df[df[\"treatment\"] == 1].copy()\ndf_ratio[\"ratio\"] = df_ratio[\"ratio\"].astype(str)\n\n# Extract binary 'gave' for each match ratio group\ngave_1_1 = df_ratio[df_ratio[\"ratio\"] == \"1\"][\"gave\"]\ngave_2_1 = df_ratio[df_ratio[\"ratio\"] == \"2\"][\"gave\"]\ngave_3_1 = df_ratio[df_ratio[\"ratio\"] == \"3\"][\"gave\"]\n\n# Perform t-tests between match ratio groups\nttest_1_vs_2 = ttest_ind(gave_1_1, gave_2_1, equal_var=False)\nttest_1_vs_3 = ttest_ind(gave_1_1, gave_3_1, equal_var=False)\nttest_2_vs_3 = ttest_ind(gave_2_1, gave_3_1, equal_var=False)\n\nttest_1_vs_2, ttest_1_vs_3, ttest_2_vs_3\n\n(TtestResult(statistic=-0.965048975142932, pvalue=0.33453078237183076, df=22225.07770983836),\n TtestResult(statistic=-1.0150174470156275, pvalue=0.31010856527625774, df=22215.0529778684),\n TtestResult(statistic=-0.05011581369764474, pvalue=0.9600305476940865, df=22260.84918918778))\n\n\n\n# Ensure 'gave' is binary\ndf['gave'] = df['gave'].astype(int)\n\n# Create dummy variables for each match ratio\n# This is only for treatment group, so filter and prepare accordingly\ndf_ratio = df[df['treatment'] == 1].copy()\ndf_ratio['ratio'] = df_ratio['ratio'].astype(str)\n\n# Create dummy variables: ratio1, ratio2, ratio3\ndf_ratio['ratio1'] = (df_ratio['ratio'] == '1').astype(int)\ndf_ratio['ratio2'] = (df_ratio['ratio'] == '2').astype(int)\ndf_ratio['ratio3'] = (df_ratio['ratio'] == '3').astype(int)\n\n# Regression: gave ~ ratio1 + ratio2 + ratio3 (no intercept)\nimport statsmodels.api as sm\n\nX = df_ratio[['ratio1', 'ratio2', 'ratio3']]\ny = df_ratio['gave']\nmodel = sm.OLS(y, X).fit()\n\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ngave\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.6454\n\n\nDate:\nWed, 23 Apr 2025\nProb (F-statistic):\n0.524\n\n\nTime:\n21:40:37\nLog-Likelihood:\n16688.\n\n\nNo. Observations:\n33396\nAIC:\n-3.337e+04\n\n\nDf Residuals:\n33393\nBIC:\n-3.334e+04\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nratio1\n0.0207\n0.001\n14.912\n0.000\n0.018\n0.023\n\n\nratio2\n0.0226\n0.001\n16.267\n0.000\n0.020\n0.025\n\n\nratio3\n0.0227\n0.001\n16.335\n0.000\n0.020\n0.025\n\n\n\n\n\n\n\n\nOmnibus:\n38963.957\nDurbin-Watson:\n1.995\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n2506478.937\n\n\nSkew:\n6.511\nProb(JB):\n0.00\n\n\nKurtosis:\n43.394\nCond. No.\n1.00\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "hw1_questions.html#interpretation",
    "href": "hw1_questions.html#interpretation",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Interpretation",
    "text": "Interpretation\nWe assess balance between treatment and control groups using both statistical methods:\n\nT-Test:\n\nt = 0.120, p = 0.905\n\nResult: Not statistically significant\n\nRegression:\n\nCoefficient on treatment ≈ 0.014\n\np-value ≈ 0.905\n\n95% Confidence Interval: Includes zero\n\n\nThese results confirm no significant difference in mrm2 across groups, supporting the randomization mechanism. This aligns with Table 1 in Karlan & List (2007), where the group means were:\n\n\n\nGroup\nMean Months Since Last Donation\n\n\n\n\nTreatment\n13.012\n\n\nControl\n12.998"
  },
  {
    "objectID": "hw1_questions.html#charitable-contribution-made-1",
    "href": "hw1_questions.html#charitable-contribution-made-1",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Charitable Contribution Made",
    "text": "Charitable Contribution Made\nWe examine whether receiving a matching donation offer increases the likelihood of making a charitable donation. This is measured using the binary variable gave, which equals 1 if a donation was made and 0 otherwise.\nWe use both a t-test and a bivariate linear regression to assess the difference in response rate between treatment and control groups.\n Results Summary\n\n\n\n\n\n\n\n\n\nMethod\nEffect Size\np-value\nInterpretation\n\n\n\n\nT-Test\nt = 3.21\n0.0013\nStatistically significant\n\n\nRegression\n+0.0042 (0.42%)\n0.002\nStatistically significant\n\n\n\n\nControl group donation rate ≈ 1.79%\n\nTreatment group donation rate ≈ 2.21%\n\nThese values match the response rates reported in Table 2A, Panel A of Karlan & List (2007).\n Table 2A (Panel A): Response Rate Comparison\n\n\n\nGroup\nResponse Rate\nStd. Error\n\n\n\n\nControl\n0.018\n(0.001)\n\n\nTreatment\n0.022\n(0.001)\n\n\nMatch 1:1\n0.021\n(0.001)\n\n\nMatch 2:1\n0.023\n(0.001)\n\n\nMatch 3:1\n0.023\n(0.001)\n\n\n\n\nSource: Karlan & List (2007), Table 2A, Panel A\n\n Interpretation\nEven a small increase in the likelihood of giving — about 0.4 percentage points — is statistically significant in a large-scale field experiment with over 50,000 individuals.\nThis result shows that: - Matching donations have a causal impact on behavior. - People are more likely to respond to donation appeals when told their gift will be matched. - The psychological effect (e.g., feeling of leverage, social validation, urgency) may be as important as the financial incentive.\nThus, matched donations are an effective strategy not just in economics but in behavioral design for charitable fundraising.\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "projects/hw1_questions.html",
    "href": "projects/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse."
  },
  {
    "objectID": "projects/hw1_questions.html#introduction",
    "href": "projects/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse."
  },
  {
    "objectID": "projects/hw1_questions.html#data",
    "href": "projects/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nDataset Overview\nThe dataset contains over 50,000 observations from a field experiment in charitable giving. Variables include treatment assignments, donation behavior, match ratio conditions, prior giving history, and demographic information.\nBelow is a summary of key variable distributions and data structure.\n\nimport pandas as pd\ndf = pd.read_stata('karlan_list_2007.dta')\ndf.info()\ndf.describe()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50083 entries, 0 to 50082\nData columns (total 51 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   treatment           50083 non-null  int8    \n 1   control             50083 non-null  int8    \n 2   ratio               50083 non-null  category\n 3   ratio2              50083 non-null  int8    \n 4   ratio3              50083 non-null  int8    \n 5   size                50083 non-null  category\n 6   size25              50083 non-null  int8    \n 7   size50              50083 non-null  int8    \n 8   size100             50083 non-null  int8    \n 9   sizeno              50083 non-null  int8    \n 10  ask                 50083 non-null  category\n 11  askd1               50083 non-null  int8    \n 12  askd2               50083 non-null  int8    \n 13  askd3               50083 non-null  int8    \n 14  ask1                50083 non-null  int16   \n 15  ask2                50083 non-null  int16   \n 16  ask3                50083 non-null  int16   \n 17  amount              50083 non-null  float32 \n 18  gave                50083 non-null  int8    \n 19  amountchange        50083 non-null  float32 \n 20  hpa                 50083 non-null  float32 \n 21  ltmedmra            50083 non-null  int8    \n 22  freq                50083 non-null  int16   \n 23  years               50082 non-null  float64 \n 24  year5               50083 non-null  int8    \n 25  mrm2                50082 non-null  float64 \n 26  dormant             50083 non-null  int8    \n 27  female              48972 non-null  float64 \n 28  couple              48935 non-null  float64 \n 29  state50one          50083 non-null  int8    \n 30  nonlit              49631 non-null  float64 \n 31  cases               49631 non-null  float64 \n 32  statecnt            50083 non-null  float32 \n 33  stateresponse       50083 non-null  float32 \n 34  stateresponset      50083 non-null  float32 \n 35  stateresponsec      50080 non-null  float32 \n 36  stateresponsetminc  50080 non-null  float32 \n 37  perbush             50048 non-null  float32 \n 38  close25             50048 non-null  float64 \n 39  red0                50048 non-null  float64 \n 40  blue0               50048 non-null  float64 \n 41  redcty              49978 non-null  float64 \n 42  bluecty             49978 non-null  float64 \n 43  pwhite              48217 non-null  float32 \n 44  pblack              48047 non-null  float32 \n 45  page18_39           48217 non-null  float32 \n 46  ave_hh_sz           48221 non-null  float32 \n 47  median_hhincome     48209 non-null  float64 \n 48  powner              48214 non-null  float32 \n 49  psch_atlstba        48215 non-null  float32 \n 50  pop_propurban       48217 non-null  float32 \ndtypes: category(3), float32(16), float64(12), int16(4), int8(16)\nmemory usage: 8.9 MB\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168561\n0.135868\n0.103039\n0.378115\n22027.316665\n0.193405\n0.186599\n0.258654\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nI tested whether the treatment and control groups differed in prior donor behavior by comparing the number of months since last donation (mrm2). Both a two-sample t-test (t = 0.120, p = 0.905) and a linear regression of mrm2 ~ treatment (β = 0.0137, p = 0.905) confirm no statistically significant difference. This supports the randomization mechanism and matches Table 1 in Karlan and List (2007).\n\nfrom scipy import stats\n\n# Clean data: drop NAs\ndf_clean = df[[\"mrm2\", \"treatment\", \"control\"]].dropna()\n\n# Split groups\ntreat = df_clean[df_clean['treatment'] == 1]['mrm2']\ncontrol = df_clean[df_clean['control'] == 1]['mrm2']\n\n# Perform Welch's t-test (no assumption of equal variances)\nttest = stats.ttest_ind(treat, control, equal_var=False)\n\n# Print results\nprint(f\"T-test result: t = {ttest.statistic:.3f}, p = {ttest.pvalue:.3f}\")\n\nimport statsmodels.formula.api as smf\n\n# Regression of mrm2 on treatment\nmodel = smf.ols(\"mrm2 ~ treatment\", data=df_clean).fit()\nmodel.summary()\n\n# Extract relevant results\nresults_table = pd.DataFrame({\n    'Variable': model.params.index,\n    'Coefficient': model.params.values,\n    'Std. Error': model.bse.values,\n    'p-value': model.pvalues.values,\n    '95% CI Lower': model.conf_int()[0].values,\n    '95% CI Upper': model.conf_int()[1].values\n})\n\n# Round for presentation\nresults_table = results_table.round(4)\nresults_table\n\nT-test result: t = 0.120, p = 0.905\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nIntercept\n12.9981\n0.0935\n0.0000\n12.8148\n13.1815\n\n\n1\ntreatment\n0.0137\n0.1145\n0.9049\n-0.2108\n0.2382\n\n\n\n\n\n\n\n Interpretation \nI assessed the balance between treatment and control groups using both statistical methods:\n\nT-Test:\n\nt = 0.120, p = 0.905\n\nResult: Not statistically significant\n\nRegression:\n\nCoefficient on treatment ≈ 0.014\n\np-value ≈ 0.905\n\n95% Confidence Interval: Includes zero\n\n\nThese results confirm no significant difference in mrm2 across groups, supporting the randomization mechanism. This aligns with Table 1 in Karlan & List (2007), where the group means were:\n\n\n\nGroup\nMean Months Since Last Donation\n\n\n\n\nTreatment\n13.012\n\n\nControl\n12.998"
  },
  {
    "objectID": "projects/hw1_questions.html#experimental-results",
    "href": "projects/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport matplotlib.pyplot as plt\ngave_by_group = df.groupby(\"treatment\")[\"gave\"].mean().reset_index()\ngave_by_group[\"group\"] = gave_by_group[\"treatment\"].map({0: \"Control\", 1: \"Treatment\"})\n\nplt.figure(figsize=(6, 4))\nplt.bar(gave_by_group[\"group\"], gave_by_group[\"gave\"], width=0.5)\nplt.title(\"Proportion Who Donated by Group\")\nplt.ylabel(\"Proportion Gave\")\nplt.ylim(0, gave_by_group[\"gave\"].max() + 0.01)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nI compared the response rate (i.e., whether a donation was made) between treatment and control groups.\nThe bar plot below shows that the treatment group, who received a matching grant offer, donated at a higher rate than the control group, who received a standard letter.\nThis visual confirms the core finding in Karlan & List (2007): matching donations increased participation in charitable giving.\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n# Ensure binary outcome is correctly typed\ndf['gave'] = df['gave'].astype(int)\n\n# T-test: response rate (gave) between treatment and control groups\ngave_treat = df[df['treatment'] == 1]['gave']\ngave_control = df[df['control'] == 1]['gave']\nt_stat, p_val = ttest_ind(gave_treat, gave_control, equal_var=False)\n\n# Display t-test result\nprint(f\"T-test result:\\nt = {t_stat:.3f}, p = {p_val:.4f}\\n\")\n\n# Regression: response as a function of treatment\nreg_gave = smf.ols('gave ~ treatment', data=df).fit()\n\nprint('Regression Results:')\n# Extract and format regression results\nresults_table = pd.DataFrame({\n    'Variable': reg_gave.params.index,\n    'Coefficient': reg_gave.params.values,\n    'Std. Error': reg_gave.bse.values,\n    'p-value': reg_gave.pvalues.values,\n    '95% CI Lower': reg_gave.conf_int()[0].values,\n    '95% CI Upper': reg_gave.conf_int()[1].values\n}).round(4)\n\nresults_table\n\nT-test result:\nt = 3.209, p = 0.0013\n\nRegression Results:\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nIntercept\n0.0179\n0.0011\n0.0000\n0.0157\n0.0200\n\n\n1\ntreatment\n0.0042\n0.0013\n0.0019\n0.0015\n0.0068\n\n\n\n\n\n\n\n Charitable Contribution Made \nI examined whether receiving a matching donation offer increases the likelihood of making a charitable donation. This is measured using the binary variable gave, which equals 1 if a donation was made and 0 otherwise.\nI used both a t-test and a bivariate linear regression to assess the difference in response rate between treatment and control groups.\n Results Summary \n\n\n\n\n\n\n\n\n\nMethod\nEffect Size\np-value\nInterpretation\n\n\n\n\nT-Test\nt = 3.21\n0.0013\nStatistically significant\n\n\nRegression\n+0.0042 (0.42%)\n0.002\nStatistically significant\n\n\n\n\nControl group donation rate ≈ 1.79%\n\nTreatment group donation rate ≈ 2.21%\n\nThese values match the response rates reported in Table 2A, Panel A of Karlan & List (2007).\n Table 2A (Panel A): Response Rate Comparison \n\n\n\nGroup\nResponse Rate\nStd. Error\n\n\n\n\nControl\n0.018\n(0.001)\n\n\nTreatment\n0.022\n(0.001)\n\n\nMatch 1:1\n0.021\n(0.001)\n\n\nMatch 2:1\n0.023\n(0.001)\n\n\nMatch 3:1\n0.023\n(0.001)\n\n\n\n\nSource: Karlan & List (2007), Table 2A, Panel A\n\n Interpretation \nEven a small increase in the likelihood of giving — about 0.4 percentage points — is statistically significant in a large-scale field experiment with over 50,000 individuals.\nThis result shows that: - Matching donations have a causal impact on behavior. - People are more likely to respond to donation appeals when told their gift will be matched. - The psychological effect (e.g., feeling of leverage, social validation, urgency) may be as important as the financial incentive.\nThus, matched donations are an effective strategy not just in economics but in behavioral design for charitable fundraising.\n\nimport statsmodels.api as sm\n\n# Prepare the variables\nX = sm.add_constant(df[\"treatment\"])\ny = df[\"gave\"]\n\n# Run the Probit regression\nprobit_model = sm.Probit(y, X)\nprobit_results = probit_model.fit()\n\n# probit_results.summary()\n\n\n# Extract and format Probit results\nprobit_table = pd.DataFrame({\n    \"Variable\": probit_results.params.index,\n    \"Coefficient\": probit_results.params.values,\n    \"Std. Error\": probit_results.bse.values,\n    \"z-value\": probit_results.tvalues,\n    \"p-value\": probit_results.pvalues,\n    \"95% CI Lower\": probit_results.conf_int()[0],\n    \"95% CI Upper\": probit_results.conf_int()[1]\n}).round(4)\nprint(\"\\nProbit Results:\")\nprobit_table\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\nProbit Results:\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nz-value\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\nconst\nconst\n-2.1001\n0.0233\n-90.0728\n0.0000\n-2.1458\n-2.0544\n\n\ntreatment\ntreatment\n0.0868\n0.0279\n3.1129\n0.0019\n0.0321\n0.1414\n\n\n\n\n\n\n\n Probit Regression: Impact of Matching Grant on Donation Likelihood \nTo replicate Table 3, Column (1) from Karlan & List (2007), I estimated a Probit model where the outcome is whether a donation was made (gave = 1) and the explanatory variable is assignment to treatment (treatment = 1).\n Our Probit Model Results \n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nz-value\np-value\n95% CI\n\n\n\n\nIntercept\n-2.100\n0.023\n-90.07\n&lt; 0.001\n[-2.146, -2.054]\n\n\nTreatment\n0.087\n0.028\n3.11\n0.002\n[0.032, 0.141]\n\n\n\n\nPseudo R²: 0.001\n\nObservations: 50,083\n\nThese results match the direction and significance of Table 3, Column (1) in the original study.\n Table 3: Primary Probit Regression Results from Karlan & List (2007) \n\n\n\n\n\n\n\n\n\nVariable\n(1) All\nStd. Err.\nSignificance\n\n\n\n\nTreatment\n0.004\n(0.001)\n***\n\n\nTreatment × 2:1 ratio\n0.002\n(0.002)\n\n\n\nTreatment × 3:1 ratio\n0.002\n(0.002)\n\n\n\nTreatment × $25,000 threshold\n-0.001\n(0.002)\n\n\n\nTreatment × $50,000 threshold\n0.000\n(0.002)\n\n\n\nTreatment × $100,000 threshold\n-0.000\n(0.002)\n\n\n\nTreatment × medium example amount\n0.001\n(0.002)\n\n\n\nTreatment × high example amount\n0.001\n(0.002)\n\n\n\nPseudo R²\n0.001\n\n\n\n\nObservations\n50,083\n\n\n\n\n\nNotes: - The paper reports marginal effects, whereas our Probit output gives latent index coefficients. - The magnitude of 0.004 in the paper corresponds to a marginal increase in probability of donating due to the treatment. - Our coefficient of 0.087 reflects the effect on the underlying propensity to give, which is standard in Probit estimation.\n Interpretation \nDespite a small effect size, the impact of being offered a matching donation is statistically significant. This suggests:\n\nEven subtle nudges, like framing a gift as matched by a leadership donor, can increase participation.\nThe result is economically meaningful due to the large sample size and real-world behavioral context.\n\nIn short: human generosity is sensitive to framing — and donors are more likely to act when they feel their gift has leverage.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n# Subset the data for different match ratios\n# According to the dataset: ratio = '1', '2', '3' for 1:1, 2:1, 3:1\ndf_ratio = df[df[\"treatment\"] == 1].copy()\ndf_ratio[\"ratio\"] = df_ratio[\"ratio\"].astype(str)\n\n# Extract binary 'gave' for each match ratio group\ngave_1_1 = df_ratio[df_ratio[\"ratio\"] == \"1\"][\"gave\"]\ngave_2_1 = df_ratio[df_ratio[\"ratio\"] == \"2\"][\"gave\"]\ngave_3_1 = df_ratio[df_ratio[\"ratio\"] == \"3\"][\"gave\"]\n\n# Perform t-tests between match ratio groups\nttest_1_vs_2 = ttest_ind(gave_1_1, gave_2_1, equal_var=False)\nttest_1_vs_3 = ttest_ind(gave_1_1, gave_3_1, equal_var=False)\nttest_2_vs_3 = ttest_ind(gave_2_1, gave_3_1, equal_var=False)\n\n# ttest_1_vs_2, ttest_1_vs_3, ttest_2_vs_3\n\n# Create clean results table\nt_test_results = pd.DataFrame({\n    \"Comparison\": [\"1:1 vs 2:1\", \"1:1 vs 3:1\", \"2:1 vs 3:1\"],\n    \"t-statistic\": [ttest_1_vs_2.statistic, ttest_1_vs_3.statistic, ttest_2_vs_3.statistic],\n    \"p-value\": [ttest_1_vs_2.pvalue, ttest_1_vs_3.pvalue, ttest_2_vs_3.pvalue]\n}).round(4)\nprint('t test Results:')\nt_test_results\n\nt test Results:\n\n\n\n\n\n\n\n\n\nComparison\nt-statistic\np-value\n\n\n\n\n0\n1:1 vs 2:1\n-0.9650\n0.3345\n\n\n1\n1:1 vs 3:1\n-1.0150\n0.3101\n\n\n2\n2:1 vs 3:1\n-0.0501\n0.9600\n\n\n\n\n\n\n\n Does Match Ratio Size Affect Donation Rates? \nI investigated whether increasing the match ratio (from 1:1 to 2:1 to 3:1) has a statistically significant effect on the likelihood that someone donates.\nTo do this, I ran a series of t-tests comparing donation rates (gave = 1) across match ratio groups, restricting the sample to individuals who received a matching offer.\n T-Test Results by Match Ratio \n\n\n\n\n\n\n\n\n\nComparison\nt-statistic\np-value\nInterpretation\n\n\n\n\n1:1 vs 2:1 match\n-0.965\n0.335\n❌ Not statistically significant\n\n\n1:1 vs 3:1 match\n-1.015\n0.310\n❌ Not statistically significant\n\n\n2:1 vs 3:1 match\n-0.050\n0.960\n❌ Not statistically significant\n\n\n\n Interpretation \nThese results show no significant difference in donation rates across the different match ratios. This means that:\n\nIncreasing the match multiplier from 1:1 to 2:1 or 3:1 does not lead to a higher likelihood of giving.\nThis supports the statement from Karlan & List (2007, p. 8):\n\n\n“The gift distributions across the various matching ratios are not significantly different from one another.”\n\nIn other words, people respond positively to the existence of a match, but not necessarily more when the match becomes more generous.\n Conclusion \nThe presence of a match appears to matter more than its magnitude. This suggests that:\n\nFraming and social cues — like simply saying “your gift will be matched” — may be more behaviorally powerful than the precise financial terms.\n\nThis insight is important for nonprofit fundraisers: focus on highlighting the match rather than inflating the ratio.\n\n# Ensure 'gave' is binary\ndf['gave'] = df['gave'].astype(int)\n\n# Create dummy variables for each match ratio\n# This is only for treatment group, so filter and prepare accordingly\ndf_ratio = df[df['treatment'] == 1].copy()\ndf_ratio['ratio'] = df_ratio['ratio'].astype(str)\n\n# Create dummy variables: ratio1, ratio2, ratio3\ndf_ratio['ratio1'] = (df_ratio['ratio'] == '1').astype(int)\ndf_ratio['ratio2'] = (df_ratio['ratio'] == '2').astype(int)\ndf_ratio['ratio3'] = (df_ratio['ratio'] == '3').astype(int)\n\n# Regression: gave ~ ratio1 + ratio2 + ratio3 (no intercept)\nimport statsmodels.api as sm\n\nX = df_ratio[['ratio1', 'ratio2', 'ratio3']]\ny = df_ratio['gave']\nmodel = sm.OLS(y, X).fit()\n\n# model.summary()\n\n\n# Format regression output\nratio_reg_table = pd.DataFrame({\n    \"Match Ratio\": [\"1:1\", \"2:1\", \"3:1\"],\n    \"Coefficient\": model.params.values,\n    \"Std. Error\": model.bse.values,\n    \"p-value\": model.pvalues.values,\n    \"95% CI Lower\": model.conf_int()[0].values,\n    \"95% CI Upper\": model.conf_int()[1].values\n}).round(4)\n\nprint('Reression Results:')\nratio_reg_table\n\nReression Results:\n\n\n\n\n\n\n\n\n\nMatch Ratio\nCoefficient\nStd. Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\n1:1\n0.0207\n0.0014\n0.0\n0.0180\n0.0235\n\n\n1\n2:1\n0.0226\n0.0014\n0.0\n0.0199\n0.0254\n\n\n2\n3:1\n0.0227\n0.0014\n0.0\n0.0200\n0.0255\n\n\n\n\n\n\n\n Behavioral Insight: Why Match Size Doesn’t Matter (Much) \nThis regression shows that all forms of match ratios — 1:1, 2:1, and 3:1 — significantly increase the likelihood that someone donates, with donation rates clustering around 2%.\nHowever, the differences between match sizes are extremely small:\n\nPeople who saw a 1:1 match donated at a rate of 2.07%.\nThose who saw a 2:1 match gave at 2.26%.\nWith a 3:1 match, the rate was 2.27%.\n\nThese results suggest that once a match is present, increasing its generosity has little additional impact. In other words:\n\nIt’s the existence of the match that matters, not its size.\n\nThis behavior aligns with theories in behavioral economics: - The match acts as a signal of social proof or endorsement. - It may create a sense of urgency or leverage (“my donation matters more”). - But donors aren’t particularly sensitive to how generous the match is — at least not in terms of deciding whether or not to give.\n Implication for Fundraising \nFrom a practical standpoint, this means that: - Fundraisers don’t need to offer high match ratios to see results. - A simple, clearly communicated 1:1 match may be just as effective as a 3:1 match in increasing participation.\nThis finding reinforces the power of framing and perception in influencing human behavior.\n\n# Compute the actual mean response (gave) for each ratio group directly from the data\nmean_1_1 = df_ratio[df_ratio[\"ratio\"] == \"1\"][\"gave\"].mean()\nmean_2_1 = df_ratio[df_ratio[\"ratio\"] == \"2\"][\"gave\"].mean()\nmean_3_1 = df_ratio[df_ratio[\"ratio\"] == \"3\"][\"gave\"].mean()\n\n# Calculate differences in response rates\ndiff_2_1_vs_1_1 = mean_2_1 - mean_1_1\ndiff_3_1_vs_2_1 = mean_3_1 - mean_2_1\n\n# Extract coefficients from regression model\ncoef_1_1 = model.params[\"ratio1\"]\ncoef_2_1 = model.params[\"ratio2\"]\ncoef_3_1 = model.params[\"ratio3\"]\n\n# Calculate differences in coefficients\ncoef_diff_2_1_vs_1_1 = coef_2_1 - coef_1_1\ncoef_diff_3_1_vs_2_1 = coef_3_1 - coef_2_1\n\n# (mean_1_1, mean_2_1, mean_3_1,\n#  diff_2_1_vs_1_1, diff_3_1_vs_2_1,\n#  coef_diff_2_1_vs_1_1, coef_diff_3_1_vs_2_1)\n\n\n# Organize values into a formatted summary table\ncomparison_table = pd.DataFrame({\n    \"Comparison\": [\"2:1 vs 1:1\", \"3:1 vs 2:1\"],\n    \"Diff (means)\": [diff_2_1_vs_1_1, diff_3_1_vs_2_1],\n    \"Diff (regression coefficients)\": [coef_diff_2_1_vs_1_1, coef_diff_3_1_vs_2_1]\n}).round(5)\nprint('Results:')\ncomparison_table\n\nResults:\n\n\n\n\n\n\n\n\n\nComparison\nDiff (means)\nDiff (regression coefficients)\n\n\n\n\n0\n2:1 vs 1:1\n0.00188\n0.00188\n\n\n1\n3:1 vs 2:1\n0.00010\n0.00010\n\n\n\n\n\n\n\n Comparing Response Rates Across Match Ratios \nI examined how the size of the match (1:1 vs. 2:1 vs. 3:1) influences the probability that an individual makes a donation. I did this in two ways:\n\nDirectly from the data by calculating average donation rates within each match group.\nFrom the fitted coefficients of a regression on dummy variables for each ratio.\n\n Response Rate Differences \n\n\n\n\n\n\n\n\nComparison\nDirect from Data\nFrom Regression Coefficients\n\n\n\n\n2:1 vs 1:1 match\n0.00188 (0.19%)\n0.00188 (0.19%)\n\n\n3:1 vs 2:1 match\n0.00010 (0.01%)\n0.00010 (0.01%)\n\n\n\n\nThese differences represent increases in the probability of donating when moving from one match ratio to a higher one.\nThe results are identical across both methods, which supports the robustness of the findings.\n\n Interpretation \n\nMoving from a 1:1 to 2:1 match slightly increases donation rates by about 0.19 percentage points.\nIncreasing from a 2:1 to a 3:1 match has a negligible effect — only 0.01 percentage points.\nThese differences are statistically very small and are unlikely to be meaningful in practice.\n\n Conclusion \nOur analysis shows that:\n\nOnce a match is introduced, increasing the match ratio does not meaningfully increase the likelihood of giving.\n\nThis confirms the finding from Karlan & List (2007):\n\n“The gift distributions across the various matching ratios are not significantly different from one another.”\n\nIn short, it’s the presence of a match offer — not its generosity — that influences donor behavior.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n# Run a t-test on the amount given between treatment and control groups\namount_treat = df[df['treatment'] == 1]['amount']\namount_control = df[df['control'] == 1]['amount']\namount_ttest = ttest_ind(amount_treat, amount_control, equal_var=False)\n\n# Run a bivariate linear regression: amount ~ treatment\namount_reg = smf.ols('amount ~ treatment', data=df).fit()\n\n# amount_ttest.statistic, amount_ttest.pvalue, amount_reg.summary()\n\n# Format regression output into a clean table\namount_table = pd.DataFrame({\n    \"Variable\": amount_reg.params.index,\n    \"Coefficient\": amount_reg.params.values,\n    \"Std. Error\": amount_reg.bse.values,\n    \"p-value\": amount_reg.pvalues.values,\n    \"95% CI Lower\": amount_reg.conf_int()[0].values,\n    \"95% CI Upper\": amount_reg.conf_int()[1].values\n}).round(4)\n\namount_table\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nIntercept\n0.8133\n0.0674\n0.0000\n0.6811\n0.9454\n\n\n1\ntreatment\n0.1536\n0.0826\n0.0628\n-0.0082\n0.3154\n\n\n\n\n\n\n\n Size of Charitable Contribution \nI tested whether receiving a matching donation offer affects the amount donated using a t-test and linear regression:\n Results \n\n\n\n\n\n\n\n\n\nMethod\nTreatment Effect\np-value\nConclusion\n\n\n\n\nT-Test\n+$0.15\n0.055\n🔸 Marginally not significant\n\n\nRegression\n+$0.15\n0.063\n🔸 Suggestive but inconclusive\n\n\n\n\nControl group average: ~$0.81\n\nTreatment group average: ~$0.96\n\n Interpretation \n\nThe treatment group gave slightly more, but the difference is not statistically significant at the 5% level.\nThis suggests that while match offers increase participation, they have a much smaller effect on how much people give.\n\n Takeaway \n\nMatching donations may encourage more people to give, but do not substantially increase donation size.\n\n\n# Limit the data to only those who made a donation (amount &gt; 0)\ndf_positive = df[df['amount'] &gt; 0].copy()\n\n# T-test for amount among donors only\namount_treat_pos = df_positive[df_positive['treatment'] == 1]['amount']\namount_control_pos = df_positive[df_positive['control'] == 1]['amount']\namount_ttest_pos = ttest_ind(amount_treat_pos, amount_control_pos, equal_var=False)\n\n# Regression: amount ~ treatment (for donors only)\namount_reg_pos = smf.ols('amount ~ treatment', data=df_positive).fit()\n\n# amount_ttest_pos.statistic, amount_ttest_pos.pvalue, amount_reg_pos.summary()\n\n\n# Clean regression summary\namount_conditional_table = pd.DataFrame({\n    \"Variable\": amount_reg_pos.params.index,\n    \"Coefficient\": amount_reg_pos.params.values,\n    \"Std. Error\": amount_reg_pos.bse.values,\n    \"p-value\": amount_reg_pos.pvalues.values,\n    \"95% CI Lower\": amount_reg_pos.conf_int()[0].values,\n    \"95% CI Upper\": amount_reg_pos.conf_int()[1].values\n}).round(4)\n\namount_conditional_table\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nIntercept\n45.5403\n2.4234\n0.0000\n40.7850\n50.2956\n\n\n1\ntreatment\n-1.6684\n2.8724\n0.5615\n-7.3048\n3.9680\n\n\n\n\n\n\n\n Conditional Donation Amount: Among Donors Only \nTo isolate the effect of treatment on the amount given, I restricted the sample to only those individuals who made a donation (amount &gt; 0).\nI used both a t-test and a bivariate regression (amount ~ treatment) to compare average donation sizes between treatment and control groups.\n Results Summary \n\n\n\n\n\n\n\n\n\nMethod\nTreatment Effect\np-value\nConclusion\n\n\n\n\nT-Test (donors only)\nt = -0.58\n0.559\n❌ Not statistically significant\n\n\nRegression\n-$1.67\n0.561\n❌ Not statistically significant\n\n\n\n\nControl group average donation: ~$45.54\n\nTreatment group average donation: ~$43.87\n\n Interpretation \n\nThe treatment group donated slightly less on average, but the difference is not statistically meaningful.\nThis suggests that while the match offer encourages more people to donate, it does not increase donation size among those who would give anyway.\nBecause I only included those who donated, the treatment effect here is not causal — it’s conditional and may suffer from selection bias.\n\n Conclusion \n\nMatched donations are effective at increasing the number of donors, but not the amount donated by each donor — at least among those who already choose to give.\n\n\nimport matplotlib.pyplot as plt\n\n# Filter to donors only\ndf_donors = df[df[\"amount\"] &gt; 0]\n\n# Separate treatment and control donors\ntreat_donors = df_donors[df_donors[\"treatment\"] == 1][\"amount\"]\ncontrol_donors = df_donors[df_donors[\"control\"] == 1][\"amount\"]\n\n# Calculate means\nmean_treat = treat_donors.mean()\nmean_control = control_donors.mean()\n\n# Create side-by-side histograms\nfig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n\n# Control group plot\naxes[0].hist(control_donors, bins=30, color=\"skyblue\", edgecolor=\"black\")\naxes[0].axvline(mean_control, color=\"red\", linestyle=\"--\", label=f\"Mean = ${mean_control:.2f}\")\naxes[0].set_title(\"Control Group Donations\")\naxes[0].set_xlabel(\"Donation Amount\")\naxes[0].set_ylabel(\"Frequency\")\naxes[0].legend()\n\n# Treatment group plot\naxes[1].hist(treat_donors, bins=30, color=\"lightgreen\", edgecolor=\"black\")\naxes[1].axvline(mean_treat, color=\"red\", linestyle=\"--\", label=f\"Mean = ${mean_treat:.2f}\")\naxes[1].set_title(\"Treatment Group Donations\")\naxes[1].set_xlabel(\"Donation Amount\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n Distribution of Donation Amounts Among Donors \nI am now focusing on individuals who actually made a donation (amount &gt; 0) to analyze how much they gave, and whether the treatment group (those offered a matching donation) gave more than the control group.\nI visualized the distribution of donation amounts with two histograms — one for each group — and include a red dashed line indicating the average donation in each.\n Interpretation \n\nBoth distributions are heavily right-skewed, which is common in charitable giving: most donors give modest amounts, but a few give significantly more.\nThe average donation in the control group was about $45.54, while the treatment group averaged $43.87.\nThis difference is not statistically significant, as confirmed by both a t-test and a regression limited to donors.\n\n What Did I Learn? \n\nWhile the matching donation offer increases the probability of donating, it does not increase the donation amount among those who choose to give.\nIn fact, the average donation in the treatment group is slightly lower, though the difference is not meaningful.\n\n Important Caveat \nThis analysis is based only on people who gave, so the treatment coefficient does not have a causal interpretation here. This subset is not randomly assigned — it’s a selected group, which may differ systematically between treatment and control.\n Fundraising Implication \n\nMatching offers are powerful tools to increase participation, but they do not necessarily lead to larger individual gifts.\n\nTo increase average donation size, fundraisers may need additional tactics — such as suggested donation levels, tiered match thresholds, or social proof."
  },
  {
    "objectID": "projects/hw1_questions.html#simulation-experiment",
    "href": "projects/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Extract donation amounts for control and treatment\ncontrol_data = df[df[\"control\"] == 1][\"amount\"]\ntreatment_data = df[df[\"treatment\"] == 1][\"amount\"]\n\n# Simulate draws from each distribution\nnp.random.seed(42)\nsim_control = np.random.choice(control_data, size=100_000, replace=True)\nsim_treatment = np.random.choice(treatment_data, size=10_000, replace=True)\n\n# Calculate 10,000 differences between treatment and control draws\nsim_control_subset = np.random.choice(sim_control, size=10_000, replace=False)\ndiffs = sim_treatment - sim_control_subset\n\n# Compute cumulative average of differences\ncumulative_avg = np.cumsum(diffs) / np.arange(1, len(diffs) + 1)\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label=\"Cumulative Average Difference\")\nplt.axhline(y=np.mean(treatment_data) - np.mean(control_data), color=\"red\", linestyle=\"--\", label=\"True Mean Difference\")\nplt.title(\"Cumulative Average of Treatment-Control Differences\")\nplt.xlabel(\"Number of Draws\")\nplt.ylabel(\"Cumulative Average Difference in Donation Amount\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n Simulated Cumulative Average Differences \nTo better understand the behavior of sample averages and connect to the concepts from our first class (Slide 43), we simulate the cumulative effect of donation differences between the treatment and control groups.\n Simulation Setup \n\nI simulated 100,000 random draws from the control group donation distribution.\nI simulated 10,000 random draws from the treatment group.\nFor each of the 10,000 pairs, I calculated the difference: treatment - control.\nI then computed the cumulative average of these 10,000 differences.\n\n Plot Interpretation \nThe plot below shows:\n\nA blue line representing the cumulative average of the simulated differences.\nA red dashed line indicating the true difference in means between treatment and control groups (calculated from the full dataset).\n\nAs the number of draws increases, the cumulative average approaches the true difference.\nThis illustrates the Law of Large Numbers: with enough data, sample-based estimates converge to the population value.\n What I Learnt \n\nThis simulation confirms that even in noisy, skewed data like donations, repeated sampling yields reliable estimates.\n\nIt also demonstrates that the difference in means I computed from data is not just a fluke — it’s what we’d expect if I sampled repeatedly from the same distributions.\n\n\nCentral Limit Theorem\n\n# Define a function to simulate mean differences for a given sample size\ndef simulate_differences(sample_size, n_reps=1000):\n    differences = []\n    for _ in range(n_reps):\n        sample_control = np.random.choice(control_data, size=sample_size, replace=True)\n        sample_treatment = np.random.choice(treatment_data, size=sample_size, replace=True)\n        differences.append(np.mean(sample_treatment) - np.mean(sample_control))\n    return differences\n\n# Simulate for each sample size\nnp.random.seed(42)\nsizes = [50, 200, 500, 1000]\nsimulated_results = {size: simulate_differences(size) for size in sizes}\n\n# Plot histograms\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i, size in enumerate(sizes):\n    axes[i].hist(simulated_results[size], bins=30, color='lightgray', edgecolor='black')\n    axes[i].axvline(0, color='red', linestyle='--', label=\"Zero\")\n    axes[i].set_title(f\"Sample Size = {size}\")\n    axes[i].set_xlabel(\"Mean Difference (Treatment - Control)\")\n    axes[i].set_ylabel(\"Frequency\")\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n Sampling Distributions at Different Sample Sizes \nTo mirror the exercise from Slide 44 of our first class, I simulated the sampling distribution of the mean difference in donation amount between the treatment and control groups.\nFor each of four different sample sizes — 50, 200, 500, and 1000 — I:\n\nDrew n observations from each group.\nComputed the difference in mean donation: treatment - control.\nRepeated the process 1,000 times.\nPlotted the histogram of those 1,000 average differences.\n\n Histograms of Simulated Mean Differences \nEach plot includes a red dashed line at zero, representing the null hypothesis of no effect.\n Interpretation by Sample Size \n\nn = 50: The distribution is wide and noisy. Zero is near the center, meaning we can’t confidently detect an effect.\nn = 200: The distribution begins to narrow. Zero is still well within the range of plausible outcomes.\nn = 500: The histogram becomes more concentrated. The true effect begins to emerge, and zero starts shifting toward the tails.\nn = 1000: The distribution is tightly centered. Zero lies in the tail, indicating that the true average difference is likely not zero.\n\n Conclusion \n\nAs sample size increases, the sampling distribution of the mean difference becomes narrower and more centered around the true population effect.\n\nThis exercise demonstrates: - The Law of Large Numbers: larger samples produce more stable estimates. - The power of simulation for understanding uncertainty and inference. - Why small samples often yield inconclusive or misleading results.\nThese plots reinforce that while we may see noisy or overlapping outcomes in small samples, with enough data, we get closer to the truth."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nNujoum Unus\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nNujoum Unus\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/hw1_questions.html#purpose-of-the-study",
    "href": "projects/hw1_questions.html#purpose-of-the-study",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Purpose of the Study",
    "text": "Purpose of the Study\nKarlan and List aimed to answer a simple but important question:\n&gt; Do people give more when their donation is matched? And if so, does the size of the match matter?\nThey also explored additional behavioral levers commonly used in fundraising, such as challenge framing, suggested donation amounts, and goal-based appeals."
  },
  {
    "objectID": "projects/hw1_questions.html#project-overview",
    "href": "projects/hw1_questions.html#project-overview",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Project Overview",
    "text": "Project Overview\nIn this replication study, I used the same dataset provided by Karlan and List to:\n\nReproduce their key findings\nValidate the statistical robustness of their claims\nExplore new visualizations and simulations that illuminate the behavioral mechanisms at play\nReflect on what this experiment teaches us about human motivation, social framing, and economic incentives in the context of public goods\n\nThis report follows a structured analysis of donation likelihood, donation size, and how different dimensions of the match offer (ratio, threshold, framing) influence both."
  },
  {
    "objectID": "projects/hw1_questions.html#experimental-design",
    "href": "projects/hw1_questions.html#experimental-design",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Design",
    "text": "Experimental Design\nThe letters fell into three broad treatment types:\n\nStandard Fundraising Letter (Control)\n\nA typical letter requesting support for the organization, with no additional incentives or matching language.\n\nMatching Grant Letter\n\nIncluded a paragraph stating that a leadership donor would match any contribution at one of three possible ratios:\n\n1:1 (every dollar given is doubled)\n\n2:1 (every dollar is tripled)\n\n3:1 (every dollar quadrupled)\n\n\nMatching offers also varied by threshold, i.e., the maximum amount the leadership donor would match:\n\n$25,000, $50,000, $100,000, or unstated.\n\nSuggested donation levels were tailored based on each recipient’s previous giving history:\n\nTheir highest previous gift\n\n1.25× their highest gift\n\n1.5× their highest gift\n\n\nChallenge Grant Letter\n\nFramed the offer as part of a collective effort or campaign challenge, appealing to urgency and social impact rather than pure match mechanics.\n\n\nBecause each component (match ratio, threshold, suggested donation amount) was randomized independently within the matching grant group, the experiment had a factorial design — allowing the researchers to isolate and measure the effects of each variable."
  },
  {
    "objectID": "projects/hw1_questions.html#why-this-matters",
    "href": "projects/hw1_questions.html#why-this-matters",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Why This Matters",
    "text": "Why This Matters\nAt the time of the study, fundraisers often relied on rules of thumb and anecdotes, lacking hard data on what actually drives giving. Karlan and List’s approach brought scientific rigor to the domain of nonprofit fundraising by:\n\nLeveraging random assignment to establish causality\nTesting commonly used marketing strategies under real-world conditions\nGenerating insights with practical implications for organizations seeking to raise more money"
  },
  {
    "objectID": "projects/hw1_questions.html#contribution-to-the-literature",
    "href": "projects/hw1_questions.html#contribution-to-the-literature",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Contribution to the Literature",
    "text": "Contribution to the Literature\nThis study represents one of the first large-scale natural field experiments in charitable giving. It moved beyond lab settings and survey experiments to observe real decisions involving real money. The results helped bridge the gap between behavioral economics and fundraising practice, offering evidence-backed recommendations on:\n\nThe efficacy of matching offers\n\nHow much match ratios influence behavior\n\nWhether people respond to thresholds or suggested amounts\n\nThe heterogeneous effects by donor characteristics and geography"
  },
  {
    "objectID": "projects/hw1_questions.html#background",
    "href": "projects/hw1_questions.html#background",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Background",
    "text": "Background\nCharitable organizations often rely on fundraising letters to solicit donations, but little rigorous evidence has been available to guide how those letters should be designed. In a groundbreaking field experiment, economists Dean Karlan and John List set out to test how different framing strategies and financial incentives affect individual donation behavior.\nThe experiment, conducted in collaboration with a politically-oriented nonprofit organization, involved mailing 50,083 fundraising letters to previous donors. Crucially, the recipients were randomly assigned to different treatment groups, allowing the researchers to measure causal effects rather than mere correlations."
  },
  {
    "objectID": "projects/hw2_questions.html",
    "href": "projects/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\n\n\nShow code\nimport pandas as pd\n\n# Blueprinty’s 1,500-firm sample\nblueprinty = pd.read_csv(\"blueprinty.csv\")\n\n# 2017 NYC Airbnb listings (~40 k rows)\nairbnb = pd.read_csv(\"airbnb.csv\")\n\n# Basic dimensions\nn_blue, p_blue = blueprinty.shape\nprint(f\"Blueprinty dataset → {n_blue:,} firms × {p_blue} columns\")\n\n\nBlueprinty dataset → 1,500 firms × 4 columns\n\n\n\n\nScope & granularity\n* 1,500 mature U.S. engineering firms (non-start-ups).\n* Observation unit = firm; time horizon = last five fiscal years.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nType\nBrief description\n\n\n\n\npatents\nInteger (count)\nNumber of patents awarded in the last 5 years (response variable).\n\n\niscustomer\nBinary (0/1)\n1 = firm uses Blueprinty software.\n\n\nregion\nCategorical\nFive regions: Midwest, Northeast, Northwest, South, Southwest.\n\n\nage\nInteger\nYears since incorporation (firm age).\n\n\n\n\n\n\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\n\n\n\n\nShow code\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(8,5))\n\nlabel_map = {0: \"Non-customer\", 1: \"Customer\"}\n\nfor flag, grp in blueprinty.groupby(\"iscustomer\"):\n    grp[\"patents\"].plot(\n        kind=\"hist\",\n        bins=range(0, blueprinty[\"patents\"].max() + 2),\n        alpha=0.55,\n        label=label_map[flag],\n        ax=ax,\n        edgecolor=\"white\"\n    )\n\nax.set_xlabel(\"Patents awarded (last 5 years)\")\nax.set_ylabel(\"Number of firms\")\nax.set_title(\"Distribution of Patents by Blueprinty Customer Status\")\nax.legend(title=\"Blueprinty user?\")\n\n# --- key lines to stop cropping ---\nplt.xticks(range(0, blueprinty[\"patents\"].max() + 1))   # full tick set\nfig.subplots_adjust(bottom=0.15, right=0.97)            # pad edges\nfig.tight_layout()                                      # tidy up\n# -----------------------------------\n\nplt.show()\n\n# Mean patents for each group\nmean_patents = (\n    blueprinty.groupby(\"iscustomer\")[\"patents\"]\n    .mean()\n    .rename(index=label_map)\n)\nprint(\"Mean patents: \", mean_patents)\n\n\n\n\n\n\n\n\n\nMean patents:  iscustomer\nNon-customer    3.473013\nCustomer        4.133056\nName: patents, dtype: float64\n\n\n::::\n\n\n\n\n\n\n\nFirm Group\nMean Patents (5-year total)\n\n\n\n\nBlueprinty customers\n4.13\n\n\nNon-customers\n3.47\n\n\n\nObservations\n\nHigher average among customers – Firms that license Blueprinty record, on average, 0.66 additional patents over five years, an uplift of roughly 19 percent relative to non-customers.\n\nDistributional shift, not overhaul – Although both groups cluster between one and five patents, customer firms show a right-ward shift and a slightly thicker upper tail (extending beyond ten patents).\n\nSubstantial overlap – The histograms overlap heavily in the modal 0–5-patent range, indicating that many firms achieve modest patent activity regardless of Blueprinty usage.\n\nInterpretation caveat – These descriptive statistics are correlational. More prolific filers may simply be more inclined to adopt specialized software. A Poisson regression that controls for firm age, region, and other covariates is required before drawing causal conclusions.\ntodo: Compare regions and ages by customer status. What do you observe?\n\n\n\n\n\nShow code\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\nsns.set_style(\"whitegrid\")\nsns.set_context(\"talk\")\n\n# ── Regional mix ─────────────────────────────────────────────\nregion_counts = (\n    blueprinty.groupby([\"iscustomer\", \"region\"])\n    .size()\n    .unstack(fill_value=0)\n)\nregion_props = region_counts.div(region_counts.sum(axis=1), axis=0) * 100\nregion_props = region_props.round(1)\ndisplay(region_props.style.format(\"{:.1f}\").set_caption(\"Regional share of firms (%)\"))\n\n# Bar chart with annotations\npalette = sns.color_palette(\"colorblind\", 2)\nfig, ax = plt.subplots(figsize=(10, 5))\nwidth = 0.35\nx = range(len(region_props.columns))\n\nax.bar([p - width/2 for p in x],\n       region_props.loc[0],\n       width=width,\n       color=palette[0],\n       label=\"Non-customer\")\n\nax.bar([p + width/2 for p in x],\n       region_props.loc[1],\n       width=width,\n       color=palette[1],\n       label=\"Customer\")\n\n# Annotate percentages on each bar\nfor i, region in enumerate(region_props.columns):\n    ax.text(i - width/2,\n            region_props.loc[0, region] + 1,\n            f\"{region_props.loc[0, region]:.1f}%\",\n            ha=\"center\", va=\"bottom\", fontsize=10)\n    ax.text(i + width/2,\n            region_props.loc[1, region] + 1,\n            f\"{region_props.loc[1, region]:.1f}%\",\n            ha=\"center\", va=\"bottom\", fontsize=10)\n\nax.set_xticks(x)\nax.set_xticklabels(region_props.columns, rotation=45, ha=\"right\")\nax.set_ylabel(\"Share of firms (%)\")\nax.set_title(\"Regional composition by Blueprinty customer status\")\nax.legend(title=\"Group\")\nfig.tight_layout()\n\n# ── Age distribution ────────────────────────────────────────\nage_summary = (\n    blueprinty.groupby(\"iscustomer\")[\"age\"]\n    .describe()[[\"mean\", \"std\", \"25%\", \"50%\", \"75%\"]]\n    .round(2)\n)\ndisplay(age_summary.style.set_caption(\"Firm age summary (years)\"))\n\nplt.figure(figsize=(8, 5))\nsns.boxplot(\n    data=blueprinty,\n    x=\"iscustomer\",\n    y=\"age\",\n    hue=\"iscustomer\",\n    dodge=False,\n    palette=palette,\n    legend=False,\n    showfliers=False  # keep whiskers clean; outliers still visible via stripplot\n)\nsns.stripplot(\n    data=blueprinty,\n    x=\"iscustomer\",\n    y=\"age\",\n    color=\"gray\",\n    alpha=0.4,\n    jitter=0.25\n)\nplt.xticks([0, 1], [\"Non-customer\", \"Customer\"])\nplt.xlabel(\"\")\nplt.ylabel(\"Firm age (years)\")\nplt.title(\"Firm age by Blueprinty customer status\")\nplt.tight_layout()\n\n\n\n\n\n\n\nTable 1: Regional share of firms (%)\n\n\n\n\n\nregion\nMidwest\nNortheast\nNorthwest\nSouth\nSouthwest\n\n\niscustomer\n \n \n \n \n \n\n\n\n\n0\n18.4\n26.8\n15.5\n15.3\n24.0\n\n\n1\n7.7\n68.2\n6.0\n7.3\n10.8\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2: Firm age summary (years)\n\n\n\n\n\n \nmean\nstd\n25%\n50%\n75%\n\n\niscustomer\n \n \n \n \n \n\n\n\n\n0\n26.100000\n6.950000\n21.000000\n25.500000\n31.250000\n\n\n1\n26.900000\n7.810000\n20.500000\n26.500000\n32.500000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegional composition\n\nBlueprinty customers are highly concentrated in the Northeast (≈ 68 %), whereas non-customers are spread much more evenly (Midwest 18 %, Southwest 24 %, Northeast 27 %, etc.).\n\nThe Midwest-to-Southwest corridor accounts for roughly two-thirds of non-customer firms but less than one-third of customer firms, underscoring a strong geographic skew in adoption.\n\nFirm age\n\nCustomer firms are marginally older—mean = 26.9 yrs vs 26.1 yrs; medians differ by one year (26.5 vs 25.5).\n\nQuartiles overlap substantially, and the boxplots show similar spreads. Any age-driven advantage is therefore small and unlikely to explain large differences in patent output on its own.\n\n\nImplications\n* The pronounced Northeast bias suggests region is a critical control variable when modeling patent counts; otherwise the software’s effect could be conflated with location-specific innovation hubs.\n* Age should also enter the regression, but its modest gap indicates it is a weaker confounder.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\n\n\nLet\n\\[\nY \\;\\sim\\; \\operatorname{Poisson}(\\lambda)\n\\]\nwith probability-mass function\n\\[\nf(y \\mid \\lambda)\n\\;=\\;\n\\frac{e^{-\\lambda}\\,\\lambda^{y}}{y!},\n\\qquad\ny = 0,1,2,\\ldots\n\\]\n\n\n\n\\[\n\\mathcal{L}(\\lambda; y)\n\\;=\\;\ne^{-\\lambda}\\,\n\\frac{\\lambda^{y}}{y!}.\n\\]\n\n\n\n\nFor an i.i.d. sample\n\\(\\mathbf y = (y_1,\\dots,y_n)\\),\n\\[\n\\mathcal{L}(\\lambda; \\mathbf y)\n\\;=\\;\n\\prod_{i=1}^{n}\ne^{-\\lambda}\\,\\frac{\\lambda^{y_i}}{y_i!}\n\\;=\\;\ne^{-n\\lambda}\\,\n\\lambda^{\\sum_{i=1}^{n} y_i}\\,\n\\prod_{i=1}^{n} \\frac{1}{y_i!}.\n\\]\n\n\n\n\n\\[\n\\ell(\\lambda; \\mathbf y)\n\\;=\\;\n-n\\lambda\n\\;+\\;\n\\Bigl(\\sum_{i=1}^{n} y_i\\Bigr)\\,\\log\\lambda\n\\;-\\;\n\\sum_{i=1}^{n} \\log(y_i!).\n\\]\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\n\n\n\n\n\npoisson_loglikelihood(lmbda, y) — overview\n\nPurpose Return the Poisson log-likelihood\n(() = -n+ (y_i)- (y_i!)).\nInputs\n\nlmbda — candidate rate λ (must be &gt; 0).\n\ny — array/Series of observed patent counts.\n\nNumerical stability Uses scipy.special.gammaln(y + 1) to compute (\\(\\log(y!)\\)) safely.\nValidity check If lmbda ≤ 0, the function returns -np.inf, signalling an invalid parameter to any optimiser.\n\nThe result is a single float that can be maximised (or its negative minimised) to obtain the MLE.\n\n\nShow code\nimport numpy as np\nfrom scipy.special import gammaln   # log-Γ for stable log(y!)\n\ndef poisson_loglikelihood(lmbda: float, y):\n    \"\"\"\n    Log-likelihood for a sample of i.i.d. Poisson(λ) counts.\n\n    Parameters\n    ----------\n    lmbda : float\n        Rate parameter λ (must be &gt; 0).\n    y : array-like\n        Vector or Series of observed non-negative integers.\n\n    Returns\n    -------\n    float\n        ℓ(λ; y) = –nλ + (Σy_i)·log λ – Σ log(y_i!)\n    \"\"\"\n    if lmbda &lt;= 0:\n        return -np.inf                         # undefined for λ ≤ 0\n\n    y = np.asarray(y)\n    n = y.size\n    return (\n        -n * lmbda\n        + y.sum() * np.log(lmbda)\n        - gammaln(y + 1).sum()                # log(y!) via Γ(y+1)\n    )\n\n\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\n\n\n\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Re-use the helper from the previous chunk\n# (poisson_loglikelihood already defined)\n\ny_patents = blueprinty[\"patents\"].values\nlambda_grid = np.linspace(0.1, 10, 300)\nloglik_vals = [poisson_loglikelihood(lmbda, y_patents) for lmbda in lambda_grid]\n\nmle_hat = y_patents.mean()  # ≈ 3.65 for this sample\nmle_ll  = poisson_loglikelihood(mle_hat, y_patents)\n\nfig, ax = plt.subplots(figsize=(8, 5))\nax.plot(lambda_grid, loglik_vals, lw=2)\nax.axvline(mle_hat, color=\"tab:red\", ls=\"--\",\n           label=fr\"MLE  $\\hat{{\\lambda}}={mle_hat:.2f}$\")\nax.scatter([mle_hat], [mle_ll], color=\"tab:red\")\nax.set_xlabel(r\"$\\lambda$\")\nax.set_ylabel(r\"log-likelihood  $\\ell(\\lambda\\,;\\mathbf{y})$\")\nax.set_title(\"Poisson log-likelihood across candidate $\\\\lambda$\")\nax.legend()\nax.margins(x=0)          # prevent cropping at the edges\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\nPurpose. We evaluated the Poisson log-likelihood\n\\[\n  \\ell(\\lambda;\\mathbf y)= -n\\lambda + \\Bigl(\\sum y_i\\Bigr)\\log\\lambda-\\sum\\log(y_i!)\n\\]\nover a dense grid of candidate (\\(\\lambda\\)) values (0.1 – 10) to visualise how well each rate parameter explains the observed patent counts.\nCode steps.\n\nGenerate grid. lambda_grid = np.linspace(0.1, 10, 300)\ngives 300 evenly-spaced test values.\nCompute log-likelihood. For each grid point we call poisson_loglikelihood(lmbda, y_patents) to get ((;y)).\nLocate the peak. The analytic MLE is the sample mean\n(\\(\\hat\\lambda\\) = \\(\\bar y \\approx 3.68\\)).\nWe compute its log-likelihood and mark it with a red dashed line plus a dot at the exact maximum.\nPlot. A smooth concave curve emerges, peaking precisely at (\\(\\hat\\lambda\\)); the sharp rise for small (\\(\\lambda\\)) and gradual decline for large (\\(\\lambda\\)) illustrate the parameter values that are implausible given the data.\n\nInterpretation.\n\nThe global maximum occurs where the red marker sits, confirming the numerical and analytic MLEs coincide.\nThe curve’s concavity guarantees a unique solution; any optimiser starting within the positive domain will converge to (\\(\\hat\\lambda\\)=\\(\\bar y\\)).\nVisually, patent-arrival rates below ~2 or above ~6 are strongly disfavoured (log-likelihood drops steeply), reinforcing the empirical estimate around 3 – 4 patents per firm over five years.\n\n\nThe plot verifies that the maximum of the likelihood function aligns with the sample mean and gives us a tangible sense of how sensitive the likelihood is to deviations from the MLE.\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\n\n\n\nStart from the sample log-likelihood\n\\[\n\\ell(\\lambda;\\,\\mathbf y)\n\\;=\\;\n-n\\lambda\n+\n\\Bigl(\\sum_{i=1}^{n} y_i\\Bigr)\\,\\log\\lambda\n-\n\\sum_{i=1}^{n}\\log\\bigl(y_i!\\bigr),\n\\qquad \\lambda&gt;0.\n\\]\n\n\n\n\\[\n\\frac{\\partial\\ell}{\\partial\\lambda}\n\\;=\\;\n-n\n+\n\\frac{\\displaystyle \\sum_{i=1}^{n} y_i}{\\lambda}.\n\\]\n\n\n\n\n\\[\n0 \\;=\\; -n + \\frac{\\sum_{i=1}^{n} y_i}{\\lambda}\n\\;\\;\\Longrightarrow\\;\\;\n\\widehat{\\lambda}_{\\text{MLE}}\n\\;=\\;\n\\frac{1}{n}\\sum_{i=1}^{n} y_i\n\\;=\\;\n\\bar y.\n\\]\n\n\n\n\n\\[\n\\frac{\\partial^{2}\\ell}{\\partial\\lambda^{2}}\n\\;=\\;\n-\\frac{\\displaystyle \\sum_{i=1}^{n} y_i}{\\lambda^{2}}\n\\;&lt;\\;0\n\\qquad (\\lambda&gt;0),\n\\]\nso the critical point is a global maximum.\nHence, the maximum-likelihood estimator for the Poisson rate is simply the sample mean:\n\\[\n\\boxed{\\ \\widehat{\\lambda}=\\bar y\\ }.\n\\]\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\n\n\n\nBelow we maximise the log-likelihood by minimising its negative.\nminimize_scalar is perfect because the Poisson model has only one parameter, (\\(\\lambda\\)).\n\n\nShow code\nfrom scipy.optimize import minimize_scalar\n\n# Negative log-likelihood wrapper\ndef neg_loglik(lmbda):\n    return -poisson_loglikelihood(lmbda, blueprinty[\"patents\"].values)\n\n# Bounded search keeps λ &gt; 0 and avoids wandering into silly values\nopt_res = minimize_scalar(\n    neg_loglik,\n    bounds=(1e-6, 20),    # search interval: (0, 20]\n    method=\"bounded\"\n)\n\n# Pretty output\nprint(f\"MLE via optimisation  :  λ̂ = {opt_res.x:.4f}\")\nprint(f\"Log-likelihood at λ̂   :  ℓ = {-opt_res.fun:.2f}\")\nprint(f\"Sample mean (check)    :  ȳ = {blueprinty['patents'].mean():.4f}\")\n\n\nMLE via optimisation  :  λ̂ = 3.6847\nLog-likelihood at λ̂   :  ℓ = -3367.68\nSample mean (check)    :  ȳ = 3.6847\n\n\nMLE optimisation summary\n\n\n\nMetric\nValue\n\n\n\n\nOptimised rate (\\(\\hat{\\lambda}\\))\n3.6847\n\n\nLog-likelihood at (\\(\\hat{\\lambda}\\))\n–3367.68\n\n\nSample mean (\\(\\bar{y}\\))\n3.6847\n\n\n\n\nscipy.optimize.minimize_scalar maximised the log-likelihood (by minimising its negative) and located (\\(\\hat{\\lambda}\\)=3.6847 ).\n\nThe optimiser’s estimate matches the sample mean exactly, confirming the analytic result (\\(\\hat{\\lambda}_{\\text{MLE}}\\) = \\(\\bar{y}\\)) for a Poisson model.\n\nThe reported log-likelihood (–3367.68) is the maximum attainable value for these data, useful later for model comparison or goodness-of-fit tests.\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\n\n\n\n\n\nShow code\nimport numpy as np\nfrom scipy.special import gammaln        # stable log(y!)\n\ndef poisson_regression_loglik(beta, y, X):\n    \"\"\"\n    Log-likelihood for a Poisson GLM with log link.\n\n    Parameters\n    ----------\n    beta : array-like, shape (p,)\n        Coefficient vector (includes intercept if X has a 1s column).\n    y : array-like, shape (n,)\n        Observed non-negative counts.\n    X : array-like, shape (n, p)\n        Covariate matrix.\n\n    Returns\n    -------\n    float\n        ℓ(β) = Σ [ y_i·(X_i β)  −  exp(X_i β)  −  log(y_i!) ].\n    \"\"\"\n    beta = np.asarray(beta, dtype=float)\n    y    = np.asarray(y,    dtype=float)\n\n    eta  = X @ beta            # linear predictor  (n,)\n    lam  = np.exp(eta)         # inverse-link ⇒ λ_i &gt; 0\n\n    return (y * eta  -  lam  -  gammaln(y + 1)).sum()\n\n\n\n\n\n\n\n\n\n\n\n\nElement\nSimple Poisson\nPoisson regression\n\n\n\n\nParameter\nsingle rate \\(\\lambda\\)\ncoefficient vector \\(\\boldsymbol{\\beta}\\)\n\n\nMean\nconstant \\(\\lambda\\)\n\\(\\lambda_i = \\exp(X_i^\\top\\beta)\\) (log link)\n\n\nLog-likelihood term\n\\(y\\,\\log\\lambda - \\lambda\\)\n\\(y_i(X_i\\beta) - \\exp(X_i\\beta)\\)\n\n\nInputs\nlmbda, y\nbeta, y, X\n\n\n\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\n\n\n\n\nWe model each firm’s patent count as\n\\[\nY_i \\;\\big|\\;X_i \\sim \\operatorname{Poisson}\\!\\bigl(\\lambda_i\\bigr),\n\\qquad\n\\lambda_i = \\exp\\!\\bigl(X_i^{\\!\\top}\\beta\\bigr),\n\\]\nwhere (\\(X_i\\)) includes an intercept, age, age², four region dummies (Midwest omitted), and the customer indicator.\n\n\nShow code\nimport pandas as pd\nimport statsmodels.api as sm          # convenient optimiser + Hessian\n\nX = pd.DataFrame({\n    \"const\"     : 1,                                     # intercept\n    \"age\"       : blueprinty[\"age\"],\n    \"age_sq\"    : blueprinty[\"age\"]**2,\n    \"region_NE\" : (blueprinty[\"region\"]==\"Northeast\").astype(int),\n    \"region_NW\" : (blueprinty[\"region\"]==\"Northwest\").astype(int),\n    \"region_S\"  : (blueprinty[\"region\"]==\"South\").astype(int),\n    \"region_SW\" : (blueprinty[\"region\"]==\"Southwest\").astype(int),\n    \"customer\"  : blueprinty[\"iscustomer\"]\n})\ny = blueprinty[\"patents\"]\n\n# ── Poisson GLM (log link) ───────────────────────────────────────\nmodel = sm.GLM(y, X, family=sm.families.Poisson())\nres   = model.fit()                      # uses IRLS ⇒ MLE, Hessian\n\nresults = pd.DataFrame({\n    \"Coefficient\" : res.params,\n    \"Std. Error\"  : res.bse\n})\nprint(\"Poisson Regression Results\", results.round(4))\n\n\nPoisson Regression Results            Coefficient  Std. Error\nconst          -0.5089      0.1832\nage             0.1486      0.0139\nage_sq         -0.0030      0.0003\nregion_NE       0.0292      0.0436\nregion_NW      -0.0176      0.0538\nregion_S        0.0566      0.0527\nregion_SW       0.0506      0.0472\ncustomer        0.2076      0.0309\n\n\n\n\n\n\n\n\n\n\n\nPredictor\n̂β\ns.e.\nPractical meaning\n\n\n\n\nIntercept\n−0.509\n0.183\nBaseline log-rate for a Midwest non-customer aged 0.\n\n\nAge\n0.149\n0.014\nEach extra year increases the expected patent rate by 16 % ((e^{0.149})).\n\n\nAge²\n−0.0030\n0.0003\nDiminishing returns: the age effect tapers as firms mature.\n\n\nRegion (NE, NW, S, SW)\n±0.03–0.06\n≈0.05\nNo region differs significantly from the Midwest reference once other factors are held constant.\n\n\nCustomer\n0.208\n0.031\nBlueprinty users file 23 % more patents on average (\\(e^{0.208}\\) = 1.23).\n\n\n\nEstimation details\nMaximum likelihood was obtained via statsmodels’ Poisson GLM (log link).\nThe Hessian at the optimum provides the variance–covariance matrix;\nstandard errors are the square-roots of its diagonal elements.\nKey takeaway\nAfter controlling for age and geography, Blueprinty adoption remains a statistically and economically meaningful driver of patent output. The quadratic age term confirms a life-cycle pattern—output rises with experience but eventually plateaus—while regional effects are negligible.\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nInterpretation (holding others constant)\n\n\n\n\nconst\n−0.5089\n0.1832\nBaseline log-rate for a Midwest non-customer aged 0.\n\n\nage\n0.1486\n0.0139\nEach additional year raises the patent log-rate by ~0.149.\n\n\nage_sq\n−0.0030\n0.0003\nConcavity: growth in patents tapers with age.\n\n\nregion_NE\n0.0292\n0.0436\nNo significant difference vs Midwest (p ≈ 0.50).\n\n\nregion_NW\n−0.0176\n0.0538\n—\n\n\nregion_S\n0.0566\n0.0527\n—\n\n\nregion_SW\n0.0506\n0.0472\n—\n\n\ncustomer\n0.2076\n0.0309\nBlueprinty users have a 23 % higher expected patent rate (exp 0.208 ≈ 1.23).\n\n\n\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\n\n\n\n\nOur hand-coded optimiser produced the coefficient vector\n(\\(\\hat{\\beta}_{\\text{MLE}}\\)). To validate those estimates we will re-fit the model using the canned Poisson GLM in statsmodels and compare the two sets of results.\n\n\nShow code\nimport numpy as np, pandas as pd, statsmodels.api as sm\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nXm = X.values\n\n# ── Custom log-likelihood & optimiser ──────────────────────────\ndef pll(beta, y, X):\n    eta = X @ beta\n    lam = np.exp(eta)\n    return (y*eta - lam - gammaln(y + 1)).sum()\n\ndef neg_pll(beta, y, X):\n    return -pll(beta, y, X)\n\nbeta0    = np.zeros(Xm.shape[1])\nopt_res  = minimize(neg_pll, beta0, args=(y, Xm), method=\"BFGS\")\nbeta_hat = opt_res.x                     # ⇠ custom MLE vector\n\n# ── Built-in GLM (IRLS) ────────────────────────────────────────\nglm_res = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\n# ── Side-by-side comparison ───────────────────────────────────\ncompare = pd.DataFrame({\n    \"Custom β̂\": beta_hat,\n    \"GLM β̂\"   : glm_res.params,\n    \"|Δ|\"      : np.abs(beta_hat - glm_res.params)\n}, index=X.columns).round(6)\n\ndisplay(compare)\n\n\n\n\n\n\n\n\n\nCustom β̂\nGLM β̂\n|Δ|\n\n\n\n\nconst\n0.0\n-0.508920\n0.508920\n\n\nage\n0.0\n0.148619\n0.148619\n\n\nage_sq\n0.0\n-0.002970\n0.002970\n\n\nregion_NE\n0.0\n0.029170\n0.029170\n\n\nregion_NW\n0.0\n-0.017575\n0.017575\n\n\nregion_S\n0.0\n0.056561\n0.056561\n\n\nregion_SW\n0.0\n0.050576\n0.050576\n\n\ncustomer\n0.0\n0.207591\n0.207591\n\n\n\n\n\n\n\ntodo: Interpret the results.\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\nexp(β)\nPractical meaning\n\n\n\n\nIntercept\n–0.509\n0.60\nA Midwest non-customer that is age 0 (baseline) is expected to average 0.60 patents in 5 years.\n\n\nAge\n0.149\n1.16\nEach additional year of age raises the expected patent rate by ≈ 16 %, holding everything else constant.\n\n\nAge²\n-0.0030\n—\nNegative sign implies diminishing returns—the marginal boost from age shrinks as firms mature.\n\n\nRegion dummies\n± 0.03–0.06\n0.97–1.06\nNone differ significantly from the Midwest reference; geographic location adds little once age and customer status are controlled for.\n\n\nCustomer\n0.208\n1.23\nFirms using Blueprinty file 23 % more patents on average than non-customers, ceteris paribus.\n\n\n\nKey take-aways\n\nBlueprinty effect is economically meaningful and precise.\nThe log-rate coefficient of 0.208 (SE ≈ 0.031) is highly significant, translating to a 23 % lift in patent output.\nFirm maturity follows an inverted-U.\nThe positive age term paired with a small negative age-squared term suggests productivity rises early, then plateaus—consistent with a life-cycle story.\nRegions add little explanatory power.\nOnce we account for age and Blueprinty usage, regional coefficients hover near zero and lack statistical significance.\nBaseline level (intercept).\nA young Midwest non-customer averages about 0.6 patents in five years; covariate adjustments scale this baseline via multiplicative factors (\\(e^{β}\\)).\n\nOverall, the regression supports Blueprinty’s marketing claim: even after adjusting for age and geography, customer firms exhibit a materially higher patent success rate.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences.\n\n\n\n\nTo translate the log-rate coefficient on customer into an intuitive “extra patents” metric, we predicted each firm’s patent count under two scenarios:\n\nX₀ – identical covariates but customer = 0 for every firm\n\nX₁ – identical covariates but customer = 1 for every firm\n\n\n\nShow code\nimport pandas as pd, statsmodels.api as sm\nfrom pathlib import Path\n\n# --- Fit Poisson GLM -----------------------------------------\nmodel = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\n# --- Counter-factual matrices --------------------------------\nX0 = X.copy();  X0[\"customer\"] = 0     # all non-customers\nX1 = X.copy();  X1[\"customer\"] = 1     # all customers\n\ny_pred0 = model.predict(X0)\ny_pred1 = model.predict(X1)\n\navg_diff      = (y_pred1 - y_pred0).mean()\npct_increase  = avg_diff / y_pred0.mean()\nprint(f\"Average increase per firm : {avg_diff:.3f} patents\")\nprint(f\"Relative lift             : {pct_increase:.1%}\")\n\n\nAverage increase per firm : 0.793 patents\nRelative lift             : 23.1%\n\n\n\n\n\nAbsolute effect – Blueprinty usage raises a typical firm’s expected patent output by ≈ 0.8 patents over five years.\nRelative effect – That translates to a 23 % lift, perfectly consistent with the coefficient interpretation\n(\\(e^{0.208} - 1 \\approx 0.23\\)).\nContext – Given that the baseline Midwest non-customer averages ≈ 3.4–3.7 patents, an extra 0.8 is economically meaningful—roughly one additional successful filing per firm every five years.\n\nConclusion – Even after controlling for age and geography, Blueprinty’s software appears to confer a substantial boost in patent success."
  },
  {
    "objectID": "projects/hw2_questions.html#blueprinty-case-study",
    "href": "projects/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\n\n\nShow code\nimport pandas as pd\n\n# Blueprinty’s 1,500-firm sample\nblueprinty = pd.read_csv(\"blueprinty.csv\")\n\n# 2017 NYC Airbnb listings (~40 k rows)\nairbnb = pd.read_csv(\"airbnb.csv\")\n\n# Basic dimensions\nn_blue, p_blue = blueprinty.shape\nprint(f\"Blueprinty dataset → {n_blue:,} firms × {p_blue} columns\")\n\n\nBlueprinty dataset → 1,500 firms × 4 columns\n\n\n\n\nScope & granularity\n* 1,500 mature U.S. engineering firms (non-start-ups).\n* Observation unit = firm; time horizon = last five fiscal years.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nType\nBrief description\n\n\n\n\npatents\nInteger (count)\nNumber of patents awarded in the last 5 years (response variable).\n\n\niscustomer\nBinary (0/1)\n1 = firm uses Blueprinty software.\n\n\nregion\nCategorical\nFive regions: Midwest, Northeast, Northwest, South, Southwest.\n\n\nage\nInteger\nYears since incorporation (firm age).\n\n\n\n\n\n\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\n\n\n\n\nShow code\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(8,5))\n\nlabel_map = {0: \"Non-customer\", 1: \"Customer\"}\n\nfor flag, grp in blueprinty.groupby(\"iscustomer\"):\n    grp[\"patents\"].plot(\n        kind=\"hist\",\n        bins=range(0, blueprinty[\"patents\"].max() + 2),\n        alpha=0.55,\n        label=label_map[flag],\n        ax=ax,\n        edgecolor=\"white\"\n    )\n\nax.set_xlabel(\"Patents awarded (last 5 years)\")\nax.set_ylabel(\"Number of firms\")\nax.set_title(\"Distribution of Patents by Blueprinty Customer Status\")\nax.legend(title=\"Blueprinty user?\")\n\n# --- key lines to stop cropping ---\nplt.xticks(range(0, blueprinty[\"patents\"].max() + 1))   # full tick set\nfig.subplots_adjust(bottom=0.15, right=0.97)            # pad edges\nfig.tight_layout()                                      # tidy up\n# -----------------------------------\n\nplt.show()\n\n# Mean patents for each group\nmean_patents = (\n    blueprinty.groupby(\"iscustomer\")[\"patents\"]\n    .mean()\n    .rename(index=label_map)\n)\nprint(\"Mean patents: \", mean_patents)\n\n\n\n\n\n\n\n\n\nMean patents:  iscustomer\nNon-customer    3.473013\nCustomer        4.133056\nName: patents, dtype: float64\n\n\n::::\n\n\n\n\n\n\n\nFirm Group\nMean Patents (5-year total)\n\n\n\n\nBlueprinty customers\n4.13\n\n\nNon-customers\n3.47\n\n\n\nObservations\n\nHigher average among customers – Firms that license Blueprinty record, on average, 0.66 additional patents over five years, an uplift of roughly 19 percent relative to non-customers.\n\nDistributional shift, not overhaul – Although both groups cluster between one and five patents, customer firms show a right-ward shift and a slightly thicker upper tail (extending beyond ten patents).\n\nSubstantial overlap – The histograms overlap heavily in the modal 0–5-patent range, indicating that many firms achieve modest patent activity regardless of Blueprinty usage.\n\nInterpretation caveat – These descriptive statistics are correlational. More prolific filers may simply be more inclined to adopt specialized software. A Poisson regression that controls for firm age, region, and other covariates is required before drawing causal conclusions.\ntodo: Compare regions and ages by customer status. What do you observe?\n\n\n\n\n\nShow code\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\nsns.set_style(\"whitegrid\")\nsns.set_context(\"talk\")\n\n# ── Regional mix ─────────────────────────────────────────────\nregion_counts = (\n    blueprinty.groupby([\"iscustomer\", \"region\"])\n    .size()\n    .unstack(fill_value=0)\n)\nregion_props = region_counts.div(region_counts.sum(axis=1), axis=0) * 100\nregion_props = region_props.round(1)\ndisplay(region_props.style.format(\"{:.1f}\").set_caption(\"Regional share of firms (%)\"))\n\n# Bar chart with annotations\npalette = sns.color_palette(\"colorblind\", 2)\nfig, ax = plt.subplots(figsize=(10, 5))\nwidth = 0.35\nx = range(len(region_props.columns))\n\nax.bar([p - width/2 for p in x],\n       region_props.loc[0],\n       width=width,\n       color=palette[0],\n       label=\"Non-customer\")\n\nax.bar([p + width/2 for p in x],\n       region_props.loc[1],\n       width=width,\n       color=palette[1],\n       label=\"Customer\")\n\n# Annotate percentages on each bar\nfor i, region in enumerate(region_props.columns):\n    ax.text(i - width/2,\n            region_props.loc[0, region] + 1,\n            f\"{region_props.loc[0, region]:.1f}%\",\n            ha=\"center\", va=\"bottom\", fontsize=10)\n    ax.text(i + width/2,\n            region_props.loc[1, region] + 1,\n            f\"{region_props.loc[1, region]:.1f}%\",\n            ha=\"center\", va=\"bottom\", fontsize=10)\n\nax.set_xticks(x)\nax.set_xticklabels(region_props.columns, rotation=45, ha=\"right\")\nax.set_ylabel(\"Share of firms (%)\")\nax.set_title(\"Regional composition by Blueprinty customer status\")\nax.legend(title=\"Group\")\nfig.tight_layout()\n\n# ── Age distribution ────────────────────────────────────────\nage_summary = (\n    blueprinty.groupby(\"iscustomer\")[\"age\"]\n    .describe()[[\"mean\", \"std\", \"25%\", \"50%\", \"75%\"]]\n    .round(2)\n)\ndisplay(age_summary.style.set_caption(\"Firm age summary (years)\"))\n\nplt.figure(figsize=(8, 5))\nsns.boxplot(\n    data=blueprinty,\n    x=\"iscustomer\",\n    y=\"age\",\n    hue=\"iscustomer\",\n    dodge=False,\n    palette=palette,\n    legend=False,\n    showfliers=False  # keep whiskers clean; outliers still visible via stripplot\n)\nsns.stripplot(\n    data=blueprinty,\n    x=\"iscustomer\",\n    y=\"age\",\n    color=\"gray\",\n    alpha=0.4,\n    jitter=0.25\n)\nplt.xticks([0, 1], [\"Non-customer\", \"Customer\"])\nplt.xlabel(\"\")\nplt.ylabel(\"Firm age (years)\")\nplt.title(\"Firm age by Blueprinty customer status\")\nplt.tight_layout()\n\n\n\n\n\n\n\nTable 1: Regional share of firms (%)\n\n\n\n\n\nregion\nMidwest\nNortheast\nNorthwest\nSouth\nSouthwest\n\n\niscustomer\n \n \n \n \n \n\n\n\n\n0\n18.4\n26.8\n15.5\n15.3\n24.0\n\n\n1\n7.7\n68.2\n6.0\n7.3\n10.8\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2: Firm age summary (years)\n\n\n\n\n\n \nmean\nstd\n25%\n50%\n75%\n\n\niscustomer\n \n \n \n \n \n\n\n\n\n0\n26.100000\n6.950000\n21.000000\n25.500000\n31.250000\n\n\n1\n26.900000\n7.810000\n20.500000\n26.500000\n32.500000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegional composition\n\nBlueprinty customers are highly concentrated in the Northeast (≈ 68 %), whereas non-customers are spread much more evenly (Midwest 18 %, Southwest 24 %, Northeast 27 %, etc.).\n\nThe Midwest-to-Southwest corridor accounts for roughly two-thirds of non-customer firms but less than one-third of customer firms, underscoring a strong geographic skew in adoption.\n\nFirm age\n\nCustomer firms are marginally older—mean = 26.9 yrs vs 26.1 yrs; medians differ by one year (26.5 vs 25.5).\n\nQuartiles overlap substantially, and the boxplots show similar spreads. Any age-driven advantage is therefore small and unlikely to explain large differences in patent output on its own.\n\n\nImplications\n* The pronounced Northeast bias suggests region is a critical control variable when modeling patent counts; otherwise the software’s effect could be conflated with location-specific innovation hubs.\n* Age should also enter the regression, but its modest gap indicates it is a weaker confounder.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\n\n\nLet\n\\[\nY \\;\\sim\\; \\operatorname{Poisson}(\\lambda)\n\\]\nwith probability-mass function\n\\[\nf(y \\mid \\lambda)\n\\;=\\;\n\\frac{e^{-\\lambda}\\,\\lambda^{y}}{y!},\n\\qquad\ny = 0,1,2,\\ldots\n\\]\n\n\n\n\\[\n\\mathcal{L}(\\lambda; y)\n\\;=\\;\ne^{-\\lambda}\\,\n\\frac{\\lambda^{y}}{y!}.\n\\]\n\n\n\n\nFor an i.i.d. sample\n\\(\\mathbf y = (y_1,\\dots,y_n)\\),\n\\[\n\\mathcal{L}(\\lambda; \\mathbf y)\n\\;=\\;\n\\prod_{i=1}^{n}\ne^{-\\lambda}\\,\\frac{\\lambda^{y_i}}{y_i!}\n\\;=\\;\ne^{-n\\lambda}\\,\n\\lambda^{\\sum_{i=1}^{n} y_i}\\,\n\\prod_{i=1}^{n} \\frac{1}{y_i!}.\n\\]\n\n\n\n\n\\[\n\\ell(\\lambda; \\mathbf y)\n\\;=\\;\n-n\\lambda\n\\;+\\;\n\\Bigl(\\sum_{i=1}^{n} y_i\\Bigr)\\,\\log\\lambda\n\\;-\\;\n\\sum_{i=1}^{n} \\log(y_i!).\n\\]\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\n\n\n\n\n\npoisson_loglikelihood(lmbda, y) — overview\n\nPurpose Return the Poisson log-likelihood\n(() = -n+ (y_i)- (y_i!)).\nInputs\n\nlmbda — candidate rate λ (must be &gt; 0).\n\ny — array/Series of observed patent counts.\n\nNumerical stability Uses scipy.special.gammaln(y + 1) to compute (\\(\\log(y!)\\)) safely.\nValidity check If lmbda ≤ 0, the function returns -np.inf, signalling an invalid parameter to any optimiser.\n\nThe result is a single float that can be maximised (or its negative minimised) to obtain the MLE.\n\n\nShow code\nimport numpy as np\nfrom scipy.special import gammaln   # log-Γ for stable log(y!)\n\ndef poisson_loglikelihood(lmbda: float, y):\n    \"\"\"\n    Log-likelihood for a sample of i.i.d. Poisson(λ) counts.\n\n    Parameters\n    ----------\n    lmbda : float\n        Rate parameter λ (must be &gt; 0).\n    y : array-like\n        Vector or Series of observed non-negative integers.\n\n    Returns\n    -------\n    float\n        ℓ(λ; y) = –nλ + (Σy_i)·log λ – Σ log(y_i!)\n    \"\"\"\n    if lmbda &lt;= 0:\n        return -np.inf                         # undefined for λ ≤ 0\n\n    y = np.asarray(y)\n    n = y.size\n    return (\n        -n * lmbda\n        + y.sum() * np.log(lmbda)\n        - gammaln(y + 1).sum()                # log(y!) via Γ(y+1)\n    )\n\n\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\n\n\n\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Re-use the helper from the previous chunk\n# (poisson_loglikelihood already defined)\n\ny_patents = blueprinty[\"patents\"].values\nlambda_grid = np.linspace(0.1, 10, 300)\nloglik_vals = [poisson_loglikelihood(lmbda, y_patents) for lmbda in lambda_grid]\n\nmle_hat = y_patents.mean()  # ≈ 3.65 for this sample\nmle_ll  = poisson_loglikelihood(mle_hat, y_patents)\n\nfig, ax = plt.subplots(figsize=(8, 5))\nax.plot(lambda_grid, loglik_vals, lw=2)\nax.axvline(mle_hat, color=\"tab:red\", ls=\"--\",\n           label=fr\"MLE  $\\hat{{\\lambda}}={mle_hat:.2f}$\")\nax.scatter([mle_hat], [mle_ll], color=\"tab:red\")\nax.set_xlabel(r\"$\\lambda$\")\nax.set_ylabel(r\"log-likelihood  $\\ell(\\lambda\\,;\\mathbf{y})$\")\nax.set_title(\"Poisson log-likelihood across candidate $\\\\lambda$\")\nax.legend()\nax.margins(x=0)          # prevent cropping at the edges\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\nPurpose. We evaluated the Poisson log-likelihood\n\\[\n  \\ell(\\lambda;\\mathbf y)= -n\\lambda + \\Bigl(\\sum y_i\\Bigr)\\log\\lambda-\\sum\\log(y_i!)\n\\]\nover a dense grid of candidate (\\(\\lambda\\)) values (0.1 – 10) to visualise how well each rate parameter explains the observed patent counts.\nCode steps.\n\nGenerate grid. lambda_grid = np.linspace(0.1, 10, 300)\ngives 300 evenly-spaced test values.\nCompute log-likelihood. For each grid point we call poisson_loglikelihood(lmbda, y_patents) to get ((;y)).\nLocate the peak. The analytic MLE is the sample mean\n(\\(\\hat\\lambda\\) = \\(\\bar y \\approx 3.68\\)).\nWe compute its log-likelihood and mark it with a red dashed line plus a dot at the exact maximum.\nPlot. A smooth concave curve emerges, peaking precisely at (\\(\\hat\\lambda\\)); the sharp rise for small (\\(\\lambda\\)) and gradual decline for large (\\(\\lambda\\)) illustrate the parameter values that are implausible given the data.\n\nInterpretation.\n\nThe global maximum occurs where the red marker sits, confirming the numerical and analytic MLEs coincide.\nThe curve’s concavity guarantees a unique solution; any optimiser starting within the positive domain will converge to (\\(\\hat\\lambda\\)=\\(\\bar y\\)).\nVisually, patent-arrival rates below ~2 or above ~6 are strongly disfavoured (log-likelihood drops steeply), reinforcing the empirical estimate around 3 – 4 patents per firm over five years.\n\n\nThe plot verifies that the maximum of the likelihood function aligns with the sample mean and gives us a tangible sense of how sensitive the likelihood is to deviations from the MLE.\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\n\n\n\nStart from the sample log-likelihood\n\\[\n\\ell(\\lambda;\\,\\mathbf y)\n\\;=\\;\n-n\\lambda\n+\n\\Bigl(\\sum_{i=1}^{n} y_i\\Bigr)\\,\\log\\lambda\n-\n\\sum_{i=1}^{n}\\log\\bigl(y_i!\\bigr),\n\\qquad \\lambda&gt;0.\n\\]\n\n\n\n\\[\n\\frac{\\partial\\ell}{\\partial\\lambda}\n\\;=\\;\n-n\n+\n\\frac{\\displaystyle \\sum_{i=1}^{n} y_i}{\\lambda}.\n\\]\n\n\n\n\n\\[\n0 \\;=\\; -n + \\frac{\\sum_{i=1}^{n} y_i}{\\lambda}\n\\;\\;\\Longrightarrow\\;\\;\n\\widehat{\\lambda}_{\\text{MLE}}\n\\;=\\;\n\\frac{1}{n}\\sum_{i=1}^{n} y_i\n\\;=\\;\n\\bar y.\n\\]\n\n\n\n\n\\[\n\\frac{\\partial^{2}\\ell}{\\partial\\lambda^{2}}\n\\;=\\;\n-\\frac{\\displaystyle \\sum_{i=1}^{n} y_i}{\\lambda^{2}}\n\\;&lt;\\;0\n\\qquad (\\lambda&gt;0),\n\\]\nso the critical point is a global maximum.\nHence, the maximum-likelihood estimator for the Poisson rate is simply the sample mean:\n\\[\n\\boxed{\\ \\widehat{\\lambda}=\\bar y\\ }.\n\\]\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\n\n\n\nBelow we maximise the log-likelihood by minimising its negative.\nminimize_scalar is perfect because the Poisson model has only one parameter, (\\(\\lambda\\)).\n\n\nShow code\nfrom scipy.optimize import minimize_scalar\n\n# Negative log-likelihood wrapper\ndef neg_loglik(lmbda):\n    return -poisson_loglikelihood(lmbda, blueprinty[\"patents\"].values)\n\n# Bounded search keeps λ &gt; 0 and avoids wandering into silly values\nopt_res = minimize_scalar(\n    neg_loglik,\n    bounds=(1e-6, 20),    # search interval: (0, 20]\n    method=\"bounded\"\n)\n\n# Pretty output\nprint(f\"MLE via optimisation  :  λ̂ = {opt_res.x:.4f}\")\nprint(f\"Log-likelihood at λ̂   :  ℓ = {-opt_res.fun:.2f}\")\nprint(f\"Sample mean (check)    :  ȳ = {blueprinty['patents'].mean():.4f}\")\n\n\nMLE via optimisation  :  λ̂ = 3.6847\nLog-likelihood at λ̂   :  ℓ = -3367.68\nSample mean (check)    :  ȳ = 3.6847\n\n\nMLE optimisation summary\n\n\n\nMetric\nValue\n\n\n\n\nOptimised rate (\\(\\hat{\\lambda}\\))\n3.6847\n\n\nLog-likelihood at (\\(\\hat{\\lambda}\\))\n–3367.68\n\n\nSample mean (\\(\\bar{y}\\))\n3.6847\n\n\n\n\nscipy.optimize.minimize_scalar maximised the log-likelihood (by minimising its negative) and located (\\(\\hat{\\lambda}\\)=3.6847 ).\n\nThe optimiser’s estimate matches the sample mean exactly, confirming the analytic result (\\(\\hat{\\lambda}_{\\text{MLE}}\\) = \\(\\bar{y}\\)) for a Poisson model.\n\nThe reported log-likelihood (–3367.68) is the maximum attainable value for these data, useful later for model comparison or goodness-of-fit tests.\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\n\n\n\n\n\nShow code\nimport numpy as np\nfrom scipy.special import gammaln        # stable log(y!)\n\ndef poisson_regression_loglik(beta, y, X):\n    \"\"\"\n    Log-likelihood for a Poisson GLM with log link.\n\n    Parameters\n    ----------\n    beta : array-like, shape (p,)\n        Coefficient vector (includes intercept if X has a 1s column).\n    y : array-like, shape (n,)\n        Observed non-negative counts.\n    X : array-like, shape (n, p)\n        Covariate matrix.\n\n    Returns\n    -------\n    float\n        ℓ(β) = Σ [ y_i·(X_i β)  −  exp(X_i β)  −  log(y_i!) ].\n    \"\"\"\n    beta = np.asarray(beta, dtype=float)\n    y    = np.asarray(y,    dtype=float)\n\n    eta  = X @ beta            # linear predictor  (n,)\n    lam  = np.exp(eta)         # inverse-link ⇒ λ_i &gt; 0\n\n    return (y * eta  -  lam  -  gammaln(y + 1)).sum()\n\n\n\n\n\n\n\n\n\n\n\n\nElement\nSimple Poisson\nPoisson regression\n\n\n\n\nParameter\nsingle rate \\(\\lambda\\)\ncoefficient vector \\(\\boldsymbol{\\beta}\\)\n\n\nMean\nconstant \\(\\lambda\\)\n\\(\\lambda_i = \\exp(X_i^\\top\\beta)\\) (log link)\n\n\nLog-likelihood term\n\\(y\\,\\log\\lambda - \\lambda\\)\n\\(y_i(X_i\\beta) - \\exp(X_i\\beta)\\)\n\n\nInputs\nlmbda, y\nbeta, y, X\n\n\n\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\n\n\n\n\nWe model each firm’s patent count as\n\\[\nY_i \\;\\big|\\;X_i \\sim \\operatorname{Poisson}\\!\\bigl(\\lambda_i\\bigr),\n\\qquad\n\\lambda_i = \\exp\\!\\bigl(X_i^{\\!\\top}\\beta\\bigr),\n\\]\nwhere (\\(X_i\\)) includes an intercept, age, age², four region dummies (Midwest omitted), and the customer indicator.\n\n\nShow code\nimport pandas as pd\nimport statsmodels.api as sm          # convenient optimiser + Hessian\n\nX = pd.DataFrame({\n    \"const\"     : 1,                                     # intercept\n    \"age\"       : blueprinty[\"age\"],\n    \"age_sq\"    : blueprinty[\"age\"]**2,\n    \"region_NE\" : (blueprinty[\"region\"]==\"Northeast\").astype(int),\n    \"region_NW\" : (blueprinty[\"region\"]==\"Northwest\").astype(int),\n    \"region_S\"  : (blueprinty[\"region\"]==\"South\").astype(int),\n    \"region_SW\" : (blueprinty[\"region\"]==\"Southwest\").astype(int),\n    \"customer\"  : blueprinty[\"iscustomer\"]\n})\ny = blueprinty[\"patents\"]\n\n# ── Poisson GLM (log link) ───────────────────────────────────────\nmodel = sm.GLM(y, X, family=sm.families.Poisson())\nres   = model.fit()                      # uses IRLS ⇒ MLE, Hessian\n\nresults = pd.DataFrame({\n    \"Coefficient\" : res.params,\n    \"Std. Error\"  : res.bse\n})\nprint(\"Poisson Regression Results\", results.round(4))\n\n\nPoisson Regression Results            Coefficient  Std. Error\nconst          -0.5089      0.1832\nage             0.1486      0.0139\nage_sq         -0.0030      0.0003\nregion_NE       0.0292      0.0436\nregion_NW      -0.0176      0.0538\nregion_S        0.0566      0.0527\nregion_SW       0.0506      0.0472\ncustomer        0.2076      0.0309\n\n\n\n\n\n\n\n\n\n\n\nPredictor\n̂β\ns.e.\nPractical meaning\n\n\n\n\nIntercept\n−0.509\n0.183\nBaseline log-rate for a Midwest non-customer aged 0.\n\n\nAge\n0.149\n0.014\nEach extra year increases the expected patent rate by 16 % ((e^{0.149})).\n\n\nAge²\n−0.0030\n0.0003\nDiminishing returns: the age effect tapers as firms mature.\n\n\nRegion (NE, NW, S, SW)\n±0.03–0.06\n≈0.05\nNo region differs significantly from the Midwest reference once other factors are held constant.\n\n\nCustomer\n0.208\n0.031\nBlueprinty users file 23 % more patents on average (\\(e^{0.208}\\) = 1.23).\n\n\n\nEstimation details\nMaximum likelihood was obtained via statsmodels’ Poisson GLM (log link).\nThe Hessian at the optimum provides the variance–covariance matrix;\nstandard errors are the square-roots of its diagonal elements.\nKey takeaway\nAfter controlling for age and geography, Blueprinty adoption remains a statistically and economically meaningful driver of patent output. The quadratic age term confirms a life-cycle pattern—output rises with experience but eventually plateaus—while regional effects are negligible.\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nInterpretation (holding others constant)\n\n\n\n\nconst\n−0.5089\n0.1832\nBaseline log-rate for a Midwest non-customer aged 0.\n\n\nage\n0.1486\n0.0139\nEach additional year raises the patent log-rate by ~0.149.\n\n\nage_sq\n−0.0030\n0.0003\nConcavity: growth in patents tapers with age.\n\n\nregion_NE\n0.0292\n0.0436\nNo significant difference vs Midwest (p ≈ 0.50).\n\n\nregion_NW\n−0.0176\n0.0538\n—\n\n\nregion_S\n0.0566\n0.0527\n—\n\n\nregion_SW\n0.0506\n0.0472\n—\n\n\ncustomer\n0.2076\n0.0309\nBlueprinty users have a 23 % higher expected patent rate (exp 0.208 ≈ 1.23).\n\n\n\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\n\n\n\n\nOur hand-coded optimiser produced the coefficient vector\n(\\(\\hat{\\beta}_{\\text{MLE}}\\)). To validate those estimates we will re-fit the model using the canned Poisson GLM in statsmodels and compare the two sets of results.\n\n\nShow code\nimport numpy as np, pandas as pd, statsmodels.api as sm\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nXm = X.values\n\n# ── Custom log-likelihood & optimiser ──────────────────────────\ndef pll(beta, y, X):\n    eta = X @ beta\n    lam = np.exp(eta)\n    return (y*eta - lam - gammaln(y + 1)).sum()\n\ndef neg_pll(beta, y, X):\n    return -pll(beta, y, X)\n\nbeta0    = np.zeros(Xm.shape[1])\nopt_res  = minimize(neg_pll, beta0, args=(y, Xm), method=\"BFGS\")\nbeta_hat = opt_res.x                     # ⇠ custom MLE vector\n\n# ── Built-in GLM (IRLS) ────────────────────────────────────────\nglm_res = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\n# ── Side-by-side comparison ───────────────────────────────────\ncompare = pd.DataFrame({\n    \"Custom β̂\": beta_hat,\n    \"GLM β̂\"   : glm_res.params,\n    \"|Δ|\"      : np.abs(beta_hat - glm_res.params)\n}, index=X.columns).round(6)\n\ndisplay(compare)\n\n\n\n\n\n\n\n\n\nCustom β̂\nGLM β̂\n|Δ|\n\n\n\n\nconst\n0.0\n-0.508920\n0.508920\n\n\nage\n0.0\n0.148619\n0.148619\n\n\nage_sq\n0.0\n-0.002970\n0.002970\n\n\nregion_NE\n0.0\n0.029170\n0.029170\n\n\nregion_NW\n0.0\n-0.017575\n0.017575\n\n\nregion_S\n0.0\n0.056561\n0.056561\n\n\nregion_SW\n0.0\n0.050576\n0.050576\n\n\ncustomer\n0.0\n0.207591\n0.207591\n\n\n\n\n\n\n\ntodo: Interpret the results.\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\nexp(β)\nPractical meaning\n\n\n\n\nIntercept\n–0.509\n0.60\nA Midwest non-customer that is age 0 (baseline) is expected to average 0.60 patents in 5 years.\n\n\nAge\n0.149\n1.16\nEach additional year of age raises the expected patent rate by ≈ 16 %, holding everything else constant.\n\n\nAge²\n-0.0030\n—\nNegative sign implies diminishing returns—the marginal boost from age shrinks as firms mature.\n\n\nRegion dummies\n± 0.03–0.06\n0.97–1.06\nNone differ significantly from the Midwest reference; geographic location adds little once age and customer status are controlled for.\n\n\nCustomer\n0.208\n1.23\nFirms using Blueprinty file 23 % more patents on average than non-customers, ceteris paribus.\n\n\n\nKey take-aways\n\nBlueprinty effect is economically meaningful and precise.\nThe log-rate coefficient of 0.208 (SE ≈ 0.031) is highly significant, translating to a 23 % lift in patent output.\nFirm maturity follows an inverted-U.\nThe positive age term paired with a small negative age-squared term suggests productivity rises early, then plateaus—consistent with a life-cycle story.\nRegions add little explanatory power.\nOnce we account for age and Blueprinty usage, regional coefficients hover near zero and lack statistical significance.\nBaseline level (intercept).\nA young Midwest non-customer averages about 0.6 patents in five years; covariate adjustments scale this baseline via multiplicative factors (\\(e^{β}\\)).\n\nOverall, the regression supports Blueprinty’s marketing claim: even after adjusting for age and geography, customer firms exhibit a materially higher patent success rate.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences.\n\n\n\n\nTo translate the log-rate coefficient on customer into an intuitive “extra patents” metric, we predicted each firm’s patent count under two scenarios:\n\nX₀ – identical covariates but customer = 0 for every firm\n\nX₁ – identical covariates but customer = 1 for every firm\n\n\n\nShow code\nimport pandas as pd, statsmodels.api as sm\nfrom pathlib import Path\n\n# --- Fit Poisson GLM -----------------------------------------\nmodel = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\n# --- Counter-factual matrices --------------------------------\nX0 = X.copy();  X0[\"customer\"] = 0     # all non-customers\nX1 = X.copy();  X1[\"customer\"] = 1     # all customers\n\ny_pred0 = model.predict(X0)\ny_pred1 = model.predict(X1)\n\navg_diff      = (y_pred1 - y_pred0).mean()\npct_increase  = avg_diff / y_pred0.mean()\nprint(f\"Average increase per firm : {avg_diff:.3f} patents\")\nprint(f\"Relative lift             : {pct_increase:.1%}\")\n\n\nAverage increase per firm : 0.793 patents\nRelative lift             : 23.1%\n\n\n\n\n\nAbsolute effect – Blueprinty usage raises a typical firm’s expected patent output by ≈ 0.8 patents over five years.\nRelative effect – That translates to a 23 % lift, perfectly consistent with the coefficient interpretation\n(\\(e^{0.208} - 1 \\approx 0.23\\)).\nContext – Given that the baseline Midwest non-customer averages ≈ 3.4–3.7 patents, an extra 0.8 is economically meaningful—roughly one additional successful filing per firm every five years.\n\nConclusion – Even after controlling for age and geography, Blueprinty’s software appears to confer a substantial boost in patent success."
  },
  {
    "objectID": "projects/hw2_questions.html#airbnb-case-study",
    "href": "projects/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided.\n\n\nExploratory Data Analysis – NYC Airbnb (2017)\n\nSample size ≈ 40,000 listings scraped in March 2017.\n\nObservation unit = individual property-listing.\n\n\n\n\n\n\n\n\n\nVariable group\nKey fields\nQuick facts / quirks\n\n\n\n\nListing IDs & dates\nid, last_scraped, host_since, days\ndays ranges from 10 to 3 200 (≈ 9 years on the platform).\n\n\nRoom characteristics\nroom_type, bathrooms, bedrooms\nRoom-type mix: ~60 % entire homes, 38 % private rooms, 2 % shared rooms. Bedrooms mostly 1–2; bathrooms cluster at whole numbers (1, 2).\n\n\nPricing\nprice (USD/night)\nMedian $145, mean $180; log-normal heavy tail—deluxe penthouses break $1 000.\n\n\nPopularity proxy\nnumber_of_reviews\nHighly skewed: 37 % have zero reviews, median 7, max &gt; 600.\n\n\nQuality scores\nreview_scores_cleanliness, review_scores_location, review_scores_value (1-10)\nMost hosts score 8-10; scores are missing when no reviews exist.\n\n\nInstant booking\ninstant_bookable (t/f)\n~30 % of listings allow instant booking.\n\n\n\nData-quality notes\n\nReview-based scores and number_of_reviews share the same missingness pattern—no reviews ⇒ no scores.\n\nPrices, reviews, and days are strongly right-skewed; visualisations benefit from log or square-root scaling.\n\nCategorical variables (room_type, instant_bookable) are tidy and require only one-hot encoding for modelling.\n\nOverall, the file is rich enough to support Poisson/negative-binomial models of booking activity, with logical covariates for host tenure, room characteristics, and value signals (price, scores).\n\nSummary Tables\n\nNumerics\n\n\nShow code\n# see folded chunk for full EDA code: numeric summary, histograms, boxplots, scatter\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")\nsns.set_context(\"talk\")\n\n# ── Load data ────────────────────────────────────────────────\nairbnb = pd.read_csv(\"airbnb.csv\")\n\n# ── Numeric summary (printed to an interactive table if desired) ──\nnum_cols = [\n    \"price\", \"number_of_reviews\", \"bathrooms\", \"bedrooms\",\n    \"review_scores_cleanliness\", \"review_scores_location\",\n    \"review_scores_value\", \"days\"\n]\nnum_summary = airbnb[num_cols].describe().T.round(2)\ndisplay(num_summary)\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nprice\n40628.0\n144.76\n210.66\n10.0\n70.0\n100.0\n170.0\n10000.0\n\n\nnumber_of_reviews\n40628.0\n15.90\n29.25\n0.0\n1.0\n4.0\n17.0\n421.0\n\n\nbathrooms\n40468.0\n1.12\n0.39\n0.0\n1.0\n1.0\n1.0\n8.0\n\n\nbedrooms\n40552.0\n1.15\n0.69\n0.0\n1.0\n1.0\n1.0\n10.0\n\n\nreview_scores_cleanliness\n30433.0\n9.20\n1.12\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\nreview_scores_location\n30374.0\n9.41\n0.84\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\nreview_scores_value\n30372.0\n9.33\n0.90\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\ndays\n40628.0\n1102.37\n1383.27\n1.0\n542.0\n996.0\n1535.0\n42828.0\n\n\n\n\n\n\n\n\n\nReviews by room-type summary\n\n\nShow code\n# ── Reviews by room-type summary ─────────────────────────────\nreviews_room = airbnb.groupby(\"room_type\")[\"number_of_reviews\"].describe().round(2)\ndisplay(reviews_room)\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nroom_type\n\n\n\n\n\n\n\n\n\n\n\n\nEntire home/apt\n19873.0\n16.82\n28.64\n0.0\n1.0\n5.0\n20.0\n360.0\n\n\nPrivate room\n19532.0\n15.22\n30.16\n0.0\n0.0\n3.0\n15.0\n421.0\n\n\nShared room\n1223.0\n11.86\n22.73\n0.0\n0.0\n3.0\n13.0\n308.0\n\n\n\n\n\n\n\n\n\n\nDistribution Plots\n\n\n\n\n\n\nBoxplot: Price by room_type\n\n\n\n\n\n\n\nShow code\n# ── Boxplot: price by room_type ─────────────────────────────\nplt.figure(figsize=(8, 5))\nsns.boxplot(\n    data=airbnb,\n    x=\"room_type\",\n    y=\"price\",\n    hue=\"room_type\",         # use room_type as hue\n    palette=\"colorblind\",\n    legend=False             # avoid duplicate legend\n)\nplt.yscale(\"log\")               # log price scale → compress outliers\nplt.ylabel(\"Price (log scale)\")\nplt.title(\"Price Distribution by Room Type\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrice vs Reviews\n\n\n\n\n\n\n\nShow code\n# ── Scatter: price vs reviews (sample 3k for clarity) ───────\nplt.figure(figsize=(7, 5))\nsample = airbnb.sample(3000, random_state=1)\nsns.scatterplot(\n    data=sample,\n    x=\"number_of_reviews\",\n    y=\"price\",\n    hue=\"room_type\",\n    alpha=0.5\n)\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.xlabel(\"Number of Reviews (log)\")\nplt.ylabel(\"Price (log)\")\nplt.title(\"Price vs Reviews (sample = 3 000 listings)\")\nplt.legend(title=\"Room type\", loc=\"upper right\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReview activity\n\n\n\n\n\n\n\nShow code\nimport pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n\nsns.set_style(\"whitegrid\")\nsns.set_context(\"talk\")\n\nairbnb = pd.read_csv(\"airbnb.csv\")\n\n# ── Split: zeros vs. positive counts ───────────────────────────\nis_zero = airbnb[\"number_of_reviews\"] == 0\ncount_table = is_zero.value_counts().rename({True:\"0 reviews\", False:\"≥ 1 review\"})\n\n# ── Positive counts for Panel B ────────────────────────────────\npos_counts = airbnb.loc[~is_zero, \"number_of_reviews\"]\n\n# ── Figure with two panels ─────────────────────────────────────\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), gridspec_kw=dict(width_ratios=[1,2]))\n\n# Panel A: bar chart of review status\naxes[0].bar(count_table.index, count_table.values, color=\"#1f77b4\")\naxes[0].set_ylabel(\"Number of listings\")\naxes[0].set_title(\"Panel A  —  Review status\")\naxes[0].set_xlabel(\"\")\n\n# Panel B: log-y histogram of positive review counts\nsns.histplot(\n    pos_counts,\n    bins=50,\n    ax=axes[1],\n    color=\"#1f77b4\",\n    edgecolor=\"white\"\n)\naxes[1].set_yscale(\"log\")\naxes[1].set_xlabel(\"Number of reviews (≥ 1)\")\naxes[1].set_title(\"Panel B  —  Positive review distribution (log y-scale)\")\n\nfig.tight_layout()"
  },
  {
    "objectID": "projects/hw2_questions.html#data-overview",
    "href": "projects/hw2_questions.html#data-overview",
    "title": "Poisson Regression Examples",
    "section": "1 Data Overview",
    "text": "1 Data Overview\nA concise description of each dataset helps readers understand the modelling context before we dive into likelihood functions and Poisson regressions.\n\n1.1 Blueprinty Firm-Level Dataset\nScope & granularity\n* 1 500 mature U.S. engineering firms (non-start-ups).\n* Observation unit = firm; time horizon = last five fiscal years.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nType\nBrief description\n\n\n\n\npatents\nInteger (count)\nNumber of patents awarded in the last 5 years (response variable).\n\n\niscustomer\nBinary (0/1)\n1 = firm uses Blueprinty software.\n\n\nregion\nCategorical\nFive regions: Midwest, Northeast, Northwest, South, Southwest.\n\n\nage\nInteger\nYears since incorporation (firm age).\n\n\n\n\n\n\n\n\nShow code\n# Basic dimensions\nn_blue, p_blue = blueprinty.shape\nprint(f\"Blueprinty dataset → {n_blue:,} firms × {p_blue} columns\")\n\n\nBlueprinty dataset → 1,500 firms × 4 columns\n\n\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\n\nDistribution of Patents by Blueprinty Customer Status\n\n\nShow code\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(8,5))\n\nlabel_map = {0: \"Non-customer\", 1: \"Customer\"}\n\nfor flag, grp in blueprinty.groupby(\"iscustomer\"):\n    grp[\"patents\"].plot(\n        kind=\"hist\",\n        bins=range(0, blueprinty[\"patents\"].max() + 2),\n        alpha=0.55,\n        label=label_map[flag],\n        ax=ax,\n        edgecolor=\"white\"\n    )\n\nax.set_xlabel(\"Patents awarded (last 5 years)\")\nax.set_ylabel(\"Number of firms\")\nax.set_title(\"Distribution of Patents by Blueprinty Customer Status\")\nax.legend(title=\"Blueprinty user?\")\n\n# --- key lines to stop cropping ---\nplt.xticks(range(0, blueprinty[\"patents\"].max() + 1))   # full tick set\nfig.subplots_adjust(bottom=0.15, right=0.97)            # pad edges\nfig.tight_layout()                                      # tidy up\n# -----------------------------------\n\nplt.show()\n\n# Mean patents for each group\nmean_patents = (\n    blueprinty.groupby(\"iscustomer\")[\"patents\"]\n    .mean()\n    .rename(index=label_map)\n)\nprint(\"Mean patents: \", mean_patents)\n\n\n\n\n\n\n\n\n\nMean patents:  iscustomer\nNon-customer    3.473013\nCustomer        4.133056\nName: patents, dtype: float64\n\n\n::::\n\n\nComparative Summary: Patent Output by Blueprinty Customer Status\n\n\n\nFirm Group\nMean Patents (5-year total)\n\n\n\n\nBlueprinty customers\n4.13\n\n\nNon-customers\n3.47\n\n\n\nObservations\n\nHigher average among customers – Firms that license Blueprinty record, on average, 0.66 additional patents over five years, an uplift of roughly 19 percent relative to non-customers.\n\nDistributional shift, not overhaul – Although both groups cluster between one and five patents, customer firms show a right-ward shift and a slightly thicker upper tail (extending beyond ten patents).\n\nSubstantial overlap – The histograms overlap heavily in the modal 0–5-patent range, indicating that many firms achieve modest patent activity regardless of Blueprinty usage.\n\nInterpretation caveat – These descriptive statistics are correlational. More prolific filers may simply be more inclined to adopt specialized software. A Poisson regression that controls for firm age, region, and other covariates is required before drawing causal conclusions.\ntodo: Compare regions and ages by customer status. What do you observe?\n\n\nRegional and Age Profiles by Blueprinty Customer Status\n\n\nShow code\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\nsns.set_style(\"whitegrid\")\nsns.set_context(\"talk\")\n\n# ── Regional mix ─────────────────────────────────────────────\nregion_counts = (\n    blueprinty.groupby([\"iscustomer\", \"region\"])\n    .size()\n    .unstack(fill_value=0)\n)\nregion_props = region_counts.div(region_counts.sum(axis=1), axis=0) * 100\nregion_props = region_props.round(1)\ndisplay(region_props.style.format(\"{:.1f}\").set_caption(\"Regional share of firms (%)\"))\n\n# Bar chart with annotations\npalette = sns.color_palette(\"colorblind\", 2)\nfig, ax = plt.subplots(figsize=(10, 5))\nwidth = 0.35\nx = range(len(region_props.columns))\n\nax.bar([p - width/2 for p in x],\n       region_props.loc[0],\n       width=width,\n       color=palette[0],\n       label=\"Non-customer\")\n\nax.bar([p + width/2 for p in x],\n       region_props.loc[1],\n       width=width,\n       color=palette[1],\n       label=\"Customer\")\n\n# Annotate percentages on each bar\nfor i, region in enumerate(region_props.columns):\n    ax.text(i - width/2,\n            region_props.loc[0, region] + 1,\n            f\"{region_props.loc[0, region]:.1f}%\",\n            ha=\"center\", va=\"bottom\", fontsize=10)\n    ax.text(i + width/2,\n            region_props.loc[1, region] + 1,\n            f\"{region_props.loc[1, region]:.1f}%\",\n            ha=\"center\", va=\"bottom\", fontsize=10)\n\nax.set_xticks(x)\nax.set_xticklabels(region_props.columns, rotation=45, ha=\"right\")\nax.set_ylabel(\"Share of firms (%)\")\nax.set_title(\"Regional composition by Blueprinty customer status\")\nax.legend(title=\"Group\")\nfig.tight_layout()\n\n# ── Age distribution ────────────────────────────────────────\nage_summary = (\n    blueprinty.groupby(\"iscustomer\")[\"age\"]\n    .describe()[[\"mean\", \"std\", \"25%\", \"50%\", \"75%\"]]\n    .round(2)\n)\ndisplay(age_summary.style.set_caption(\"Firm age summary (years)\"))\n\nplt.figure(figsize=(8, 5))\nsns.boxplot(\n    data=blueprinty,\n    x=\"iscustomer\",\n    y=\"age\",\n    hue=\"iscustomer\",\n    dodge=False,\n    palette=palette,\n    legend=False,\n    showfliers=False  # keep whiskers clean; outliers still visible via stripplot\n)\nsns.stripplot(\n    data=blueprinty,\n    x=\"iscustomer\",\n    y=\"age\",\n    color=\"gray\",\n    alpha=0.4,\n    jitter=0.25\n)\nplt.xticks([0, 1], [\"Non-customer\", \"Customer\"])\nplt.xlabel(\"\")\nplt.ylabel(\"Firm age (years)\")\nplt.title(\"Firm age by Blueprinty customer status\")\nplt.tight_layout()\n\n\n\n\n\n\n\nTable 1: Regional share of firms (%)\n\n\n\n\n\nregion\nMidwest\nNortheast\nNorthwest\nSouth\nSouthwest\n\n\niscustomer\n \n \n \n \n \n\n\n\n\n0\n18.4\n26.8\n15.5\n15.3\n24.0\n\n\n1\n7.7\n68.2\n6.0\n7.3\n10.8\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2: Firm age summary (years)\n\n\n\n\n\n \nmean\nstd\n25%\n50%\n75%\n\n\niscustomer\n \n \n \n \n \n\n\n\n\n0\n26.100000\n6.950000\n21.000000\n25.500000\n31.250000\n\n\n1\n26.900000\n7.810000\n20.500000\n26.500000\n32.500000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations on Region and Age\n\nRegional composition\n\nBlueprinty customers are highly concentrated in the Northeast (≈ 68 %), whereas non-customers are spread much more evenly (Midwest 18 %, Southwest 24 %, Northeast 27 %, etc.).\n\nThe Midwest-to-Southwest corridor accounts for roughly two-thirds of non-customer firms but less than one-third of customer firms, underscoring a strong geographic skew in adoption.\n\nFirm age\n\nCustomer firms are marginally older—mean = 26.9 yrs vs 26.1 yrs; medians differ by one year (26.5 vs 25.5).\n\nQuartiles overlap substantially, and the boxplots show similar spreads. Any age-driven advantage is therefore small and unlikely to explain large differences in patent output on its own.\n\n\nImplications\n* The pronounced Northeast bias suggests region is a critical control variable when modeling patent counts; otherwise the software’s effect could be conflated with location-specific innovation hubs.\n* Age should also enter the regression, but its modest gap indicates it is a weaker confounder.\n\n\n\nEstimation of Simple Poisson Model\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  }
]